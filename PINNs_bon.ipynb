{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING AUGMENTED INTERPOLATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch loaded; device is cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "try:\n",
    "    import torchinfo\n",
    "    no_torchinfo = False\n",
    "except ModuleNotFoundError:\n",
    "    no_torchinfo = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"torch loaded; device is {device}\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINNs Advection Equation\n",
    "\n",
    "This notebook solves the advection-diffusion equation using a physics-informed neural network (PINN). The equation is given by:\n",
    "\n",
    "\\begin{cases}\n",
    "\\partial_t u + a \\partial_x u = 0 \\\\\n",
    "u(t=0, x) = u_0(x, \\mu)\n",
    "\\end{cases}\n",
    "\n",
    "where $u$ is the dependent variable, $t$ is time, $x$ is space, and $a$ is the diffusion coefficient (velocity) The initial condition is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "u_0(x) = \\exp(-(x-\\mu)^2/\\sigma)\n",
    "\\end{equation}\n",
    "\n",
    "Where $\\mu$ is the mean and $\\sigma$ is the variance. The solution is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "u(x,t) = u_0(x-a*t)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis:\n",
    "\n",
    "### Neural Network\n",
    "A neural network is a function that maps an input $X$ to an output $Y$ by performing successive linear and nonlinear transformations. The linear transformations are represented by a set of weights $W$ and biases $b$ and the nonlinear transformations are represented by activation functions $\\sigma$. The output of a neural network is given by:\n",
    "\n",
    "$$\\overline{u_\\theta}(X)=W_n\\sigma_{n-1}(W_{n-1}\\sigma_{n-2}(...(W_2(W_1X+b_1)+b_2)+..)+b_{n-1})+b_n$$\n",
    "\n",
    "We will train our NN by iteratively minimizing a loss function ($MSE$:mean squared error) in the training dataset. \n",
    "\n",
    "### Physics Informed Neural Network = NN + PDE\n",
    "\n",
    "We can use a neural network to approximate any function (Universal Approximation Theorem). In our case, we want to approximate the solution of a PDE. We can do this by minimizing the error of the PDE in a certain number of points inside our domain.\n",
    "\n",
    "$$\\overline{u_\\theta}\\approx u(x,t)$$ \n",
    "\n",
    "Since NN is a function, we can obtain its derivatives: $\\frac{\\partial \\overline{u_\\theta}}{\\partial t},\\frac{\\partial \\overline{u_\\theta}}{\\partial x}$.\n",
    "\n",
    "We assume then: $$\\overline{u_\\theta}(x,t)\\approx u(x,t,\\theta)$$ \n",
    "\n",
    "Then:\n",
    "\n",
    "$$\\left(\\frac{\\partial\\overline{u_\\theta}}{\\partial t} + a \\frac{\\partial \\overline{u_\\theta}}{\\partial x}\\right) \\approx \\left(\\frac{\\partial u}{\\partial t} + a \\frac{\\partial u}{\\partial x}\\right) =0$$\n",
    "\n",
    "And:\n",
    "\n",
    "$$\\left(\\frac{\\partial\\overline{u_\\theta}}{\\partial t} + a\\frac{\\partial \\overline{u_\\theta} }{\\partial x}\\right) \\approx 0$$\n",
    "\n",
    "We should also define the boundary conditions both in time as it follows:\n",
    "\n",
    "<!-- boundary conditions in space:\n",
    "\n",
    "$$bc_{space}(x)=x*(x-1)$$ -->\n",
    "\n",
    "boundary conditions in time:\n",
    "$$bc_{time}(t)=t$$\n",
    "\n",
    "Now we can define this function as $u_\\theta$:\n",
    "\n",
    "$${u_\\theta}(x,t)= u_{0}(x)+ bc_{time}(t) * bc_{space}(x) * \\overline{u_\\theta}(x,t)$$ \n",
    "\n",
    "If $u_\\theta \\rightarrow 0$ then our $\\overline{u_\\theta}$ would be respecting the physical law.\n",
    "\n",
    "### PINNs' Loss function\n",
    "\n",
    "We evaluate our PDE in a certain number of \"collocation points\" ($N_{coll}$) inside our domain $(x,t)$, and we try also to guess the values of $\\mu$ and $\\sigma$ of our initial condition adding also collocation points to these parameters ($N_{\\mu}$ and $N_{\\sigma}$). \n",
    "\n",
    "Then we iteratively minimize a loss function related to $u_\\theta$:\n",
    "\n",
    "$$MSE_{u_\\theta}=\\frac{1}{N_{coll}}\\sum^{N_{coll}}_{i=1}|u_\\theta(t_{coll}^i,x_{coll}^i,\\mu_{coll}^i,\\sigma_{coll}^i)|^2$$\n",
    "\n",
    "Since we know the outcome, we select $N_u$ points from our BC and IC and used them to train our network.\n",
    "\n",
    "$$MSE_{u_{exact}}=\\frac{1}{N_{u_{exact}}}\\sum^{N_{u_{exact}}}_{i=1}|u(t_{u_{exact}}^i,x_{u_{exact}}^i, \\mu_{u_{exact}}^i,\\sigma_{u_{exact}}^i\n",
    ")-\\overline{u_\\theta}(t_{u_{exact}}^i,x_{u_{exact}}^i, \\mu_{u_{exact}}^i,\\sigma_{u_{exact}}^i)|^2$$\n",
    "\n",
    "\n",
    "#### Total Loss:\n",
    "\n",
    "$$MSE=MSE_{u_{exact}}+MSE_{u_\\theta}$$\n",
    "\n",
    "We are looking to minimize:\n",
    "$$\n",
    "\\Theta* = argmin_\\Theta \\left\\{ \\frac{1}{N_{coll}}\\sum^{N_{coll}}_{i=1}|u_\\theta(t_{coll}^i,x_{coll}^i,\\mu_{coll}^i,\\sigma_{coll}^i)|^2 + \\frac{1}{N_{u_{exact}}}\\sum^{N_{u_{exact}}}_{i=1}|u(t_{u_{exact}}^i,x_{u_{exact}}^i, \\mu_{u_{exact}}^i,\\sigma_{u_{exact}}^i\n",
    ")-\\overline{u_\\theta}(t_{u_{exact}}^i,x_{u_{exact}}^i, \\mu_{u_{exact}}^i,\\sigma_{u_{exact}}^i)|^2 \\right\\} $$\n",
    "\n",
    "where $u_\\theta$ is the PDE and $u$ is the exact solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Net class represents a neural network model. It inherits from nn.DataParallel, which is a PyTorch module used for parallelizing the computation on multiple GPUs.\n",
    "\n",
    "- `__init__()`: This method is the constructor of the class. It initializes the neural network layers and defines the architecture. The network consists of four hidden layers with 30 units each, followed by an output layer with 10 units. Each layer is defined using the nn.Linear module. The double() method is used to specify the data type as double precision.\n",
    "\n",
    "- `forward(x, t, mean, variance)`: This method defines the forward pass of the neural network. It takes input tensors x, t, mean, and variance, and applies a sequence of operations to produce the output of the network. The input tensors are concatenated ant the concatenated tensor is then passed through each hidden layer, with a non-linear activation function (tanh) applied after each layer. Finally, the output layer produces the final output of the network.\n",
    "\n",
    "- `network_BC(t)`: This function serves to impose hard boundary constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.DataParallel): \n",
    "    \n",
    "    \"\"\"Defines the neural network model\"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Net, self).__init__(nn.Module)\n",
    "        self.hidden_layer1 = nn.Linear(4, 30).double() \n",
    "        self.hidden_layer2 = nn.Linear(30, 30).double()\n",
    "        self.hidden_layer3 = nn.Linear(30, 30).double()\n",
    "        self.hidden_layer4 = nn.Linear(30, 10).double()\n",
    "        self.output_layer = nn.Linear(10, 1).double()\n",
    "\n",
    "    \"\"\"Defines the sequence of operations that are applied to the input tensors to produce the output of the neural network\"\"\"\n",
    "    def forward(self, x, t, mean, variance):\n",
    "    \n",
    "        inputs = torch.cat([x,t,mean,variance], axis=1)\n",
    "        layer1_out = torch.tanh(self.hidden_layer1(inputs))\n",
    "        layer2_out = torch.tanh(self.hidden_layer2(layer1_out))\n",
    "        layer3_out = torch.tanh(self.hidden_layer3(layer2_out))\n",
    "        layer4_out = torch.tanh(self.hidden_layer4(layer3_out))\n",
    "        output = self.output_layer(layer4_out)\n",
    "        return output\n",
    "\n",
    "\"\"\"To impose hard boundary constraints\"\"\"\n",
    "def network_BC(t): #t to define u_theta\n",
    "    return t "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "The Parameters class defines the set of parameters for our PINN.\n",
    "\n",
    "- `__init__(tf, file_name)`: This method is the constructor of the class. It initializes the parameters:\n",
    "  - `xmin and xmax`: They represent the minimum and maximum values for the space domain  \n",
    "  - `tmin and tmax`: They represent the minimum and maximum values for the time domain,\n",
    "  - `a`: It represents the constant velocity of the wave (diffusion coefficient)\n",
    "  - `learning_rate`: It represents the learning rate for optimization algorithm.\n",
    "  - `file_name`: It represents the name of a file that will contain the trained model. \n",
    "  - `min_mean and  max_mean`: They represent the minimum and maximum value that the mean variable that the Gaussian initial condition can take. \n",
    "  - `min_variance and max_variance`: They represent the minimum and maximum value that the variance variable of the Gaussian initial condition can take. \n",
    "\n",
    "- `u0(x, mean, variance)`: This method defines an initial condition function u0 that takes inputs x, mean, and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "   def __init__(self, tf, file_name):\n",
    "        self.xmin = 0.\n",
    "        self.xmax = 1.\n",
    "        self.tmin = 0.\n",
    "        self.tmax = tf\n",
    "        self.a = 1.\n",
    "        self.learning_rate = 1e-3 #1e-3\n",
    "        self.file_name = file_name\n",
    "        self.min_mean = 0.4\n",
    "        self.max_mean = 0.6\n",
    "        self.min_variance = 0.01\n",
    "        self.max_variance = 0.2\n",
    "     \n",
    "   def u0(self,x,mean,variance):\n",
    "        return torch.exp(-(x-mean)**2/variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PINN \n",
    "\n",
    "The Network class represents a PINN:\n",
    "\n",
    "- `__init__(param: Parameters)`: Initializes the neural network model and loads the model if available:\n",
    "  - `parameters`: An instance of the Parameters class that holds various parameters for the network.\n",
    "- `__call__(*args)`: Calls the network and returns the output.\n",
    "- `create_network()`: Creates the neural network model using the Net class.\n",
    "- `load(file_name)`: Loads the model from a file.\n",
    "- `save(file_name, epoch, net_state, optimizer_state, loss, loss_history)`: Saves the model with the specific values passed as arguments into a file.\n",
    "- `pde(x, t, mean, variance)`: Computes the PDE using the network and returns the result.\n",
    "- `predict_u_from_torch(x, t, mean, variance)`: Predicts the value of the solution given by the neural network based on the input variables x, t, mean, and variance.\n",
    "- `random(min_value, max_value, shape, requires_grad=False, device=device)`: Generates random numbers within a range.\n",
    "- `make_data(n_data)`: Generates the data of size n_data for the training processbased on the exact solution of the PDE.\n",
    "- `make_collocation(n_collocation)`: Generates n_collocation collocation points to enforce PDE constraints during training.\n",
    "- `train(epochs, n_collocation, n_data)`: Trains the neural network using a combination of PDE constraints and data fitting.\n",
    "- `u_exact(x, t, a, xmax, u0, mean, variance, device=device)`: Computes the exact solution for the PDE.\n",
    "- `plot(t, mean, variance)`: Plots the loss history, predicted solution, and prediction error at the input variables x, t, mean, and variance.\n",
    "- `animate(mean, variance)`: Creates an animation of the solution with initial condition with inputs mean and variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "\n",
    "    def __init__(self,param: Parameters):\n",
    "\n",
    "        \"\"\"Initializes the neural network model\"\"\"\n",
    "        self.parameters = param \n",
    "        self.x_min, self.x_max = param.xmin, param.xmax\n",
    "        self.t_min, self.t_max = param.tmin, param.tmax\n",
    "        self.learning_rate = param.learning_rate\n",
    "        self.u0 = param.u0\n",
    "        self.tf = param.tmax\n",
    "        self.a = param.a\n",
    "        self.file_name = param.file_name\n",
    "        self.min_mean, self.max_mean = param.min_mean, param.max_mean\n",
    "        self.min_variance, self.max_variance = param.min_variance, param.max_variance\n",
    "\n",
    "        #To save the model\n",
    "        script_dir = os.getcwd()\n",
    "        self.file_name = os.path.join(script_dir, self.file_name)\n",
    "\n",
    "        self.create_network() #returns the current working directory\n",
    "        self.load(self.file_name)\n",
    "        \n",
    "    \"\"\"This method calls the neural network model\"\"\"\n",
    "    def __call__(self, *args):\n",
    "        return self.net(*args)\n",
    "\n",
    "    def create_network(self):\n",
    "        \"\"\"Creates the neural network model\"\"\"\n",
    "        self.net = nn.DataParallel(Net()).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=self.learning_rate) #optimization algorithm\n",
    "   \n",
    "    def load(self, file_name):\n",
    "        self.loss_history = []\n",
    "        try:\n",
    "\n",
    "            try:\n",
    "                checkpoint = checkpoint = torch.load(file_name, map_location=torch.device('cpu'))\n",
    "                \n",
    "            except RuntimeError:\n",
    "                checkpoint = torch.load(file_name, map_location=torch.device('cpu'))\n",
    "\n",
    "            self.net.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            self.loss = checkpoint['loss']\n",
    "\n",
    "            try:\n",
    "                self.loss_history = checkpoint['loss_history']\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "            self.to_be_trained = False\n",
    "            print(\"network loaded successfully\")\n",
    "    \n",
    "        except FileNotFoundError:\n",
    "\n",
    "            self.to_be_trained = True\n",
    "            print(\"network was not loaded from file: training needed\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def save(file_name, epoch, net_state, optimizer_state, loss, loss_history):\n",
    "        torch.save({\n",
    "            epoch: epoch,\n",
    "            'model_state_dict': net_state,\n",
    "            'optimizer_state_dict': optimizer_state,\n",
    "            'loss': loss,\n",
    "            'loss_history': loss_history,\n",
    "        }, file_name)\n",
    "\n",
    "    def pde(self, x, t, mean, variance):\n",
    "\n",
    "        u = self.u0(x, mean, variance) + network_BC(t) * self(x, t, mean, variance)\n",
    "        u_x = torch.autograd.grad(u.sum(), x, create_graph=True)[0] # it enables the creation of a computational graph for the gradients. A computational graph is a data structure that represents the operations performed on tensors and their dependencies, which allows for automatic differentiation and backpropagation.\n",
    "        u_t = torch.autograd.grad(u.sum(), t, create_graph=True)[0] \n",
    "        \n",
    "        return u_t + self.a * u_x\n",
    "\n",
    "    \n",
    "    \"\"\"This method predicts the value of u based on the given input variables x, t and u0\"\"\"\n",
    "\n",
    "    def predict_u_from_torch(self, x, t, mean, variance): # x tensor pour lequel on calcule la solution, t float, u0 float qu'on converti en tensor \n",
    "        ones = torch.ones(x.shape, dtype=torch.double, device=device)\n",
    "        pt_mean = mean * ones\n",
    "        pt_variance = variance * ones\n",
    "        pt_t = t * ones\n",
    "        pred = self.u0(x, pt_mean, pt_variance) + network_BC(pt_t) * self(x, pt_t, pt_mean, pt_variance)\n",
    "\n",
    "        return pred\n",
    "\n",
    "    #Pour l'instant c'est pas utilisé\n",
    "    \"\"\"This method predicts the value of u for a given input x and a mesh object\"\"\"\n",
    "    def predict_u(self, x, t, mesh): # x float t float mesh object\n",
    "\n",
    "        #We need to reshape the input x to be a column vector\n",
    "        reshaped_x = x.reshape((x.size, 1))\n",
    "        pt_x = Variable(torch.from_numpy(reshaped_x).double(), requires_grad=True).to(device)\n",
    "        reshaped_t = t.reshape((x.size, 1))\n",
    "        pt_t = Variable(torch.from_numpy(reshaped_t).double(), requires_grad=False).to(device)\n",
    "\n",
    "        u_pred = self.u0(pt_x) + network_BC(pt_t) * self(pt_x, pt_t)\n",
    "\n",
    "        return u_pred.detach().cpu().numpy().reshape(x.shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def random(min_value, max_value, shape, requires_grad=False, device=device):\n",
    "        random_numbers = torch.rand(shape, device=device, dtype=torch.double, requires_grad=requires_grad)\n",
    "        return min_value + (max_value - min_value) * random_numbers\n",
    "    \n",
    "    \"\"\"This method generates the data for the training process\"\"\"\n",
    "    def make_data(self, n_data):\n",
    "        \n",
    "        shape = (n_data, 1)\n",
    "        \"\"\"We generate for each variable a random number between the min and max value\"\"\"\n",
    "        self.x_data = Network.random(self.x_min, self.x_max, shape)\n",
    "        self.t_data = Network.random(self.t_min, self.t_max, shape)\n",
    "        self.mean_data = Network.random(self.min_mean, self.max_mean, shape)\n",
    "        self.variance_data = Network.random(self.min_variance, self.max_variance, shape)\n",
    "        \n",
    "        self.u_data_exact = Network.u_exact(self.x_data, self.t_data, self.a ,self.x_max, self.u0, self.mean_data, self.variance_data) # la solution exacte evaluée dans (xi,ti)\n",
    "        self.zeros1 = torch.zeros(shape, dtype=torch.double, device=device) \n",
    "        \n",
    "    \"\"\"This method generates the collocation points for the training process to enforce \n",
    "       the partial differential equation (PDE) constraints during training.\"\"\"\n",
    "    \n",
    "    def make_collocation(self, n_collocation):\n",
    "\n",
    "        shape = (n_collocation, 1)\n",
    "        #These variables will be used to evaluate the PDE at the collocation points.\n",
    "        #collocation points are additional points where the PDE will be evaluated to ensure that the neural network solution satisfies the PDE\n",
    "        self.x_collocation = Network.random(self.x_min, self.x_max, shape, requires_grad=True)\n",
    "        self.t_collocation = Network.random(self.t_min, self.t_max, shape, requires_grad=True)\n",
    "        self.mean_collocation = Network.random(self.min_mean, self.max_mean, shape, requires_grad=True)\n",
    "        self.variance_collocation = Network.random(self.min_variance, self.max_variance, shape, requires_grad=True)\n",
    "\n",
    "        self.zeros = torch.zeros(shape, dtype=torch.double, device=device) #This will be used as a target for the PDE constraints.\n",
    "\n",
    "    \"\"\"This method trains the neural network using a combination of PDE constraints and data fitting.\"\"\"\n",
    "    def train(self, epochs, n_collocation, n_data):\n",
    "\n",
    "        mse_cost_function = torch.nn.MSELoss()\n",
    "\n",
    "        try:\n",
    "            best_loss_value = self.loss.item()\n",
    "        except AttributeError:\n",
    "            best_loss_value = 1e10\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            self.loss = 0\n",
    "\n",
    "\n",
    "            if n_collocation > 0:\n",
    "                # Loss based on PDE constraints\n",
    "        \n",
    "                self.make_collocation(n_collocation)\n",
    "                f_out = self.pde(self.x_collocation, self.t_collocation, self.mean_collocation, self.variance_collocation)\n",
    "                mse_f = mse_cost_function(f_out, self.zeros)\n",
    "                self.loss += mse_f\n",
    "            \n",
    "\n",
    "            if n_data > 0:\n",
    "                # Loss based on data fitting (on apprend que avec des donées)\n",
    "                self.make_data(n_data)\n",
    "                u_pred = self.predict_u_from_torch(self.x_data, self.t_data, self.mean_data, self.variance_data) #reseau evaluée dans (xi,ti)\n",
    "                #on essaie de minimizer la différence entre la solution exacte et la solution du réseau calculées en (xi,ti)\n",
    "                mse_data = mse_cost_function(u_pred, self.u_data_exact) \n",
    "                \n",
    "                self.loss += mse_data\n",
    "\n",
    "            \"\"\"Losses based on a combination of PDE constraints in the interior and boundary conditions\"\"\" \n",
    "\n",
    "            \"\"\"data points\"\"\"\n",
    "            xmin_tensor = torch.full(self.x_data.size(), self.x_min, dtype=torch.double, device=device, requires_grad=True)\n",
    "            xmax_tensor = torch.full(self.x_data.size(), self.x_max, dtype=torch.double, device=device, requires_grad=True)\n",
    "            tmin_tensor = torch.full(self.t_data.size(), self.t_min, dtype=torch.double, device=device, requires_grad=True)\n",
    "            tmax_tensor = torch.full(self.t_data.size(), self.t_max, dtype=torch.double, device=device, requires_grad=True)\n",
    "          \n",
    "            \"\"\"To enforce the boundary conditions, we evaluate the neural network solution at the boundary points and compare it to the boundary values.\"\"\"\n",
    "            #boundary (periodicity) in x\n",
    "            f_boundary1 = self.predict_u_from_torch(xmin_tensor, self.t_data, self.mean_data, self.variance_data)\n",
    "            f_boundary2 = self.predict_u_from_torch(xmax_tensor, self.t_data, self.mean_data, self.variance_data)\n",
    "            f_boundaryx= torch.abs(f_boundary1 - f_boundary2)\n",
    "            mse_f_boundaryx = mse_cost_function(f_boundaryx, self.zeros1)\n",
    "\n",
    "            #boundary (periodicity) in t\n",
    "            f_boundary3 = self.predict_u_from_torch(self.x_data, tmin_tensor, self.mean_data, self.variance_data)\n",
    "            f_boundary4 = self.predict_u_from_torch(self.x_data, tmax_tensor, self.mean_data, self.variance_data)\n",
    "            f_boundaryt= torch.abs(f_boundary3 - f_boundary4)\n",
    "            mse_f_boundaryt = mse_cost_function(f_boundaryt, self.zeros1)\n",
    "\n",
    "            #Initial condition\n",
    "            f_initial = self.predict_u_from_torch(self.x_data, self.t_min, self.mean_data, self.variance_data)\n",
    "            f_initial_real= self.u0(self.x_data, self.mean_data, self.variance_data)\n",
    "            f_initiale = torch.abs(f_initial - f_initial_real)\n",
    "            mse_f_initial = mse_cost_function(f_initiale, self.zeros1)\n",
    "\n",
    "            self.loss += mse_f_initial + mse_f_boundaryx + mse_f_boundaryt  \n",
    "\n",
    "            #on fait la backpropagation et on met à jour les poids\n",
    "            self.loss.backward() # This is for computing gradients using backward propagation\n",
    "            self.optimizer.step() #This is equivalent to the update step in the gradient descent algorithm\n",
    "          \n",
    "\n",
    "            self.loss_history.append(self.loss.item())\n",
    "\n",
    "            if epoch % 500 == 0:\n",
    "                print(f\"epoch {epoch: 5d}: current loss = {self.loss.item():5.2e}\")\n",
    "                try:\n",
    "                    self.save(self.file_name, epoch, best_net, best_optimizer, best_loss, self.loss_history)\n",
    "                except NameError:\n",
    "                    pass\n",
    "\n",
    "            if self.loss.item() < best_loss_value:\n",
    "                print(f\"epoch {epoch: 5d}: best loss = {self.loss.item():5.2e}\")\n",
    "                best_loss = self.loss.clone()\n",
    "                best_loss_value = best_loss.item()\n",
    "                best_net = self.net.state_dict().copy()\n",
    "                best_optimizer = self.optimizer.state_dict().copy()\n",
    "\n",
    "        print(f\"epoch {epoch: 5d}: current loss = {self.loss.item():5.2e}\")\n",
    "\n",
    "        try:\n",
    "            self.save(self.file_name, epoch, best_net, best_optimizer, best_loss, self.loss_history)\n",
    "            self.load(self.file_name)\n",
    "\n",
    "        except UnboundLocalError:\n",
    "            pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def u_exact(x, t, a, xmax, u0, mean, variance, device=device):\n",
    "        return u0((x - a * t) % xmax, mean, variance)\n",
    "    \n",
    "    \n",
    "    def plot(self, t, mean, variance):\n",
    "\n",
    "            _, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "            ax[0].semilogy(self.loss_history)\n",
    "            ax[0].set_title(\"loss history\")\n",
    "      \n",
    "            n_visu = 10000\n",
    "\n",
    "            x = torch.linspace(0, 1, n_visu, dtype=torch.double, device=device)[:, None]\n",
    "    \n",
    "            u_pred = self.predict_u_from_torch(x, t, mean, variance)\n",
    "            u_exact = Network.u_exact(x, t, self.a, self.x_max, self.u0, mean, variance)\n",
    "\n",
    "            ax[1].plot(x.cpu(), u_exact.detach().cpu(), label=\"Exact solution\")\n",
    "            ax[1].plot(x.cpu(), u_pred.detach().cpu(), label=\"Prediction (NN)\")\n",
    "            ax[1].set_title(\"Prediction\")\n",
    "            ax[1].legend()\n",
    "\n",
    "            error = torch.abs(u_pred - u_exact).detach().cpu()\n",
    "\n",
    "            ax[2].plot(x.cpu(), error)\n",
    "    \n",
    "            ax[2].set_title(\"Prediction error\")\n",
    "\n",
    "            print(\"Error \", torch.abs(u_pred - u_exact).mean().detach().cpu().item())\n",
    "\n",
    "\n",
    "    #To save the solution in the form of a .gif file\n",
    "    def animate(self, mean, variance):\n",
    "        nb_points = 1000\n",
    "        x = torch.linspace(self.x_min, self.x_max, nb_points, dtype=torch.double, device=device)[:, None]\n",
    "        t = torch.linspace(self.t_min, self.t_max, nb_points, dtype=torch.double, device=device)[:, None]\n",
    "                \n",
    "        \"\"\"u exact\"\"\"\n",
    "        u_exact = np.zeros((nb_points, nb_points))\n",
    "        for i in range(nb_points):\n",
    "            for j in range(nb_points):\n",
    "                u_exact[i,j] = Network.u_exact(x[i], t[j], self.a, self.x_max, self.u0, mean, variance)\n",
    "\n",
    "        \"\"\"prediction\"\"\"\n",
    "\n",
    "        u = np.zeros((nb_points, nb_points))\n",
    "        for i in range(nb_points):\n",
    "            u[i, :] = self.predict_u_from_torch(x, t[i], mean, variance).detach().cpu().numpy().flatten()\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        line, = ax.plot([], color='red', label='numerical solution')\n",
    "        line_exact1, = ax.plot([], color='blue', label='exact solution')\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_xlabel('x')\n",
    "        ax.set_ylabel('u')\n",
    "        ax.set_title('Solution of the advection equation')\n",
    "        ax.legend()\n",
    "\n",
    "        def animate(frame_num):\n",
    "            line.set_data(x, u[frame_num, :])\n",
    "            line_exact1.set_data(x, u_exact[frame_num, :])\n",
    "            return line, line_exact1\n",
    "\n",
    "        anim = FuncAnimation(fig, animate, frames=nb_points, interval=120, blit=True)\n",
    "        anim.save(\"animation\" + str(mean) + \".gif\", writer='pillow')\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network was not loaded from file: training needed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch     0: current loss = 4.62e+00\n",
      "epoch     0: best loss = 4.62e+00\n",
      "epoch     4: best loss = 4.59e+00\n",
      "epoch     6: best loss = 4.57e+00\n",
      "epoch    10: best loss = 4.55e+00\n",
      "epoch    14: best loss = 4.53e+00\n",
      "epoch    18: best loss = 4.52e+00\n",
      "epoch    21: best loss = 4.50e+00\n",
      "epoch    23: best loss = 4.49e+00\n",
      "epoch    24: best loss = 4.49e+00\n",
      "epoch    25: best loss = 4.48e+00\n",
      "epoch    26: best loss = 4.46e+00\n",
      "epoch    28: best loss = 4.46e+00\n",
      "epoch    29: best loss = 4.43e+00\n",
      "epoch    30: best loss = 4.42e+00\n",
      "epoch    34: best loss = 4.41e+00\n",
      "epoch    35: best loss = 4.39e+00\n",
      "epoch    37: best loss = 4.39e+00\n",
      "epoch    38: best loss = 4.38e+00\n",
      "epoch    40: best loss = 4.35e+00\n",
      "epoch    42: best loss = 4.35e+00\n",
      "epoch    44: best loss = 4.34e+00\n",
      "epoch    46: best loss = 4.33e+00\n",
      "epoch    47: best loss = 4.32e+00\n",
      "epoch    52: best loss = 4.29e+00\n",
      "epoch    56: best loss = 4.28e+00\n",
      "epoch    57: best loss = 4.24e+00\n",
      "epoch    59: best loss = 4.21e+00\n",
      "epoch    62: best loss = 4.20e+00\n",
      "epoch    63: best loss = 4.19e+00\n",
      "epoch    66: best loss = 4.14e+00\n",
      "epoch    67: best loss = 4.14e+00\n",
      "epoch    69: best loss = 4.06e+00\n",
      "epoch    71: best loss = 4.03e+00\n",
      "epoch    74: best loss = 3.99e+00\n",
      "epoch    75: best loss = 3.95e+00\n",
      "epoch    77: best loss = 3.92e+00\n",
      "epoch    78: best loss = 3.87e+00\n",
      "epoch    79: best loss = 3.81e+00\n",
      "epoch    80: best loss = 3.77e+00\n",
      "epoch    81: best loss = 3.73e+00\n",
      "epoch    82: best loss = 3.70e+00\n",
      "epoch    83: best loss = 3.66e+00\n",
      "epoch    84: best loss = 3.60e+00\n",
      "epoch    85: best loss = 3.57e+00\n",
      "epoch    86: best loss = 3.52e+00\n",
      "epoch    87: best loss = 3.44e+00\n",
      "epoch    88: best loss = 3.39e+00\n",
      "epoch    89: best loss = 3.32e+00\n",
      "epoch    90: best loss = 3.25e+00\n",
      "epoch    91: best loss = 3.19e+00\n",
      "epoch    92: best loss = 3.14e+00\n",
      "epoch    93: best loss = 3.07e+00\n",
      "epoch    94: best loss = 3.02e+00\n",
      "epoch    95: best loss = 2.97e+00\n",
      "epoch    96: best loss = 2.88e+00\n",
      "epoch    97: best loss = 2.84e+00\n",
      "epoch    98: best loss = 2.81e+00\n",
      "epoch    99: best loss = 2.79e+00\n",
      "epoch   100: best loss = 2.73e+00\n",
      "epoch   101: best loss = 2.72e+00\n",
      "epoch   102: best loss = 2.70e+00\n",
      "epoch   103: best loss = 2.67e+00\n",
      "epoch   104: best loss = 2.65e+00\n",
      "epoch   106: best loss = 2.61e+00\n",
      "epoch   108: best loss = 2.60e+00\n",
      "epoch   109: best loss = 2.58e+00\n",
      "epoch   110: best loss = 2.57e+00\n",
      "epoch   111: best loss = 2.56e+00\n",
      "epoch   112: best loss = 2.49e+00\n",
      "epoch   114: best loss = 2.47e+00\n",
      "epoch   115: best loss = 2.44e+00\n",
      "epoch   116: best loss = 2.40e+00\n",
      "epoch   117: best loss = 2.38e+00\n",
      "epoch   118: best loss = 2.36e+00\n",
      "epoch   119: best loss = 2.34e+00\n",
      "epoch   120: best loss = 2.31e+00\n",
      "epoch   121: best loss = 2.30e+00\n",
      "epoch   122: best loss = 2.26e+00\n",
      "epoch   123: best loss = 2.24e+00\n",
      "epoch   124: best loss = 2.23e+00\n",
      "epoch   125: best loss = 2.20e+00\n",
      "epoch   127: best loss = 2.20e+00\n",
      "epoch   128: best loss = 2.16e+00\n",
      "epoch   129: best loss = 2.13e+00\n",
      "epoch   130: best loss = 2.09e+00\n",
      "epoch   132: best loss = 2.07e+00\n",
      "epoch   134: best loss = 2.05e+00\n",
      "epoch   135: best loss = 2.04e+00\n",
      "epoch   136: best loss = 2.03e+00\n",
      "epoch   137: best loss = 2.02e+00\n",
      "epoch   138: best loss = 2.00e+00\n",
      "epoch   139: best loss = 1.97e+00\n",
      "epoch   141: best loss = 1.96e+00\n",
      "epoch   142: best loss = 1.96e+00\n",
      "epoch   145: best loss = 1.95e+00\n",
      "epoch   146: best loss = 1.92e+00\n",
      "epoch   148: best loss = 1.90e+00\n",
      "epoch   150: best loss = 1.90e+00\n",
      "epoch   151: best loss = 1.87e+00\n",
      "epoch   154: best loss = 1.85e+00\n",
      "epoch   158: best loss = 1.85e+00\n",
      "epoch   159: best loss = 1.83e+00\n",
      "epoch   160: best loss = 1.82e+00\n",
      "epoch   161: best loss = 1.79e+00\n",
      "epoch   163: best loss = 1.78e+00\n",
      "epoch   164: best loss = 1.77e+00\n",
      "epoch   165: best loss = 1.75e+00\n",
      "epoch   166: best loss = 1.74e+00\n",
      "epoch   168: best loss = 1.74e+00\n",
      "epoch   169: best loss = 1.72e+00\n",
      "epoch   171: best loss = 1.69e+00\n",
      "epoch   172: best loss = 1.68e+00\n",
      "epoch   175: best loss = 1.65e+00\n",
      "epoch   176: best loss = 1.65e+00\n",
      "epoch   177: best loss = 1.62e+00\n",
      "epoch   178: best loss = 1.62e+00\n",
      "epoch   179: best loss = 1.60e+00\n",
      "epoch   180: best loss = 1.58e+00\n",
      "epoch   181: best loss = 1.56e+00\n",
      "epoch   183: best loss = 1.53e+00\n",
      "epoch   184: best loss = 1.50e+00\n",
      "epoch   185: best loss = 1.49e+00\n",
      "epoch   186: best loss = 1.49e+00\n",
      "epoch   187: best loss = 1.47e+00\n",
      "epoch   188: best loss = 1.45e+00\n",
      "epoch   189: best loss = 1.42e+00\n",
      "epoch   190: best loss = 1.41e+00\n",
      "epoch   191: best loss = 1.40e+00\n",
      "epoch   192: best loss = 1.39e+00\n",
      "epoch   193: best loss = 1.36e+00\n",
      "epoch   194: best loss = 1.35e+00\n",
      "epoch   195: best loss = 1.34e+00\n",
      "epoch   196: best loss = 1.33e+00\n",
      "epoch   197: best loss = 1.30e+00\n",
      "epoch   199: best loss = 1.28e+00\n",
      "epoch   200: best loss = 1.27e+00\n",
      "epoch   202: best loss = 1.24e+00\n",
      "epoch   203: best loss = 1.24e+00\n",
      "epoch   204: best loss = 1.22e+00\n",
      "epoch   205: best loss = 1.21e+00\n",
      "epoch   206: best loss = 1.20e+00\n",
      "epoch   207: best loss = 1.20e+00\n",
      "epoch   208: best loss = 1.18e+00\n",
      "epoch   209: best loss = 1.16e+00\n",
      "epoch   211: best loss = 1.16e+00\n",
      "epoch   212: best loss = 1.15e+00\n",
      "epoch   213: best loss = 1.11e+00\n",
      "epoch   215: best loss = 1.10e+00\n",
      "epoch   217: best loss = 1.10e+00\n",
      "epoch   218: best loss = 1.09e+00\n",
      "epoch   219: best loss = 1.08e+00\n",
      "epoch   222: best loss = 1.06e+00\n",
      "epoch   224: best loss = 1.04e+00\n",
      "epoch   227: best loss = 1.04e+00\n",
      "epoch   229: best loss = 1.03e+00\n",
      "epoch   230: best loss = 1.03e+00\n",
      "epoch   231: best loss = 1.03e+00\n",
      "epoch   232: best loss = 1.01e+00\n",
      "epoch   234: best loss = 1.01e+00\n",
      "epoch   235: best loss = 9.97e-01\n",
      "epoch   239: best loss = 9.93e-01\n",
      "epoch   241: best loss = 9.91e-01\n",
      "epoch   242: best loss = 9.89e-01\n",
      "epoch   243: best loss = 9.86e-01\n",
      "epoch   244: best loss = 9.79e-01\n",
      "epoch   245: best loss = 9.65e-01\n",
      "epoch   246: best loss = 9.48e-01\n",
      "epoch   252: best loss = 9.47e-01\n",
      "epoch   254: best loss = 9.28e-01\n",
      "epoch   259: best loss = 9.08e-01\n",
      "epoch   266: best loss = 9.06e-01\n",
      "epoch   270: best loss = 8.97e-01\n",
      "epoch   271: best loss = 8.97e-01\n",
      "epoch   272: best loss = 8.94e-01\n",
      "epoch   273: best loss = 8.89e-01\n",
      "epoch   276: best loss = 8.84e-01\n",
      "epoch   277: best loss = 8.77e-01\n",
      "epoch   279: best loss = 8.65e-01\n",
      "epoch   281: best loss = 8.61e-01\n",
      "epoch   284: best loss = 8.54e-01\n",
      "epoch   288: best loss = 8.47e-01\n",
      "epoch   289: best loss = 8.46e-01\n",
      "epoch   290: best loss = 8.39e-01\n",
      "epoch   291: best loss = 8.31e-01\n",
      "epoch   293: best loss = 8.30e-01\n",
      "epoch   294: best loss = 8.17e-01\n",
      "epoch   296: best loss = 8.11e-01\n",
      "epoch   297: best loss = 8.10e-01\n",
      "epoch   298: best loss = 8.08e-01\n",
      "epoch   299: best loss = 8.04e-01\n",
      "epoch   302: best loss = 7.95e-01\n",
      "epoch   305: best loss = 7.75e-01\n",
      "epoch   306: best loss = 7.72e-01\n",
      "epoch   308: best loss = 7.68e-01\n",
      "epoch   310: best loss = 7.54e-01\n",
      "epoch   314: best loss = 7.42e-01\n",
      "epoch   315: best loss = 7.34e-01\n",
      "epoch   318: best loss = 7.31e-01\n",
      "epoch   319: best loss = 7.20e-01\n",
      "epoch   320: best loss = 7.18e-01\n",
      "epoch   321: best loss = 7.15e-01\n",
      "epoch   322: best loss = 7.13e-01\n",
      "epoch   323: best loss = 7.07e-01\n",
      "epoch   325: best loss = 7.02e-01\n",
      "epoch   326: best loss = 6.95e-01\n",
      "epoch   327: best loss = 6.89e-01\n",
      "epoch   328: best loss = 6.78e-01\n",
      "epoch   329: best loss = 6.71e-01\n",
      "epoch   331: best loss = 6.64e-01\n",
      "epoch   332: best loss = 6.57e-01\n",
      "epoch   333: best loss = 6.52e-01\n",
      "epoch   336: best loss = 6.41e-01\n",
      "epoch   337: best loss = 6.41e-01\n",
      "epoch   339: best loss = 6.39e-01\n",
      "epoch   340: best loss = 6.19e-01\n",
      "epoch   343: best loss = 6.11e-01\n",
      "epoch   344: best loss = 5.96e-01\n",
      "epoch   347: best loss = 5.92e-01\n",
      "epoch   349: best loss = 5.86e-01\n",
      "epoch   350: best loss = 5.79e-01\n",
      "epoch   351: best loss = 5.72e-01\n",
      "epoch   353: best loss = 5.61e-01\n",
      "epoch   355: best loss = 5.59e-01\n",
      "epoch   356: best loss = 5.44e-01\n",
      "epoch   358: best loss = 5.40e-01\n",
      "epoch   359: best loss = 5.38e-01\n",
      "epoch   360: best loss = 5.34e-01\n",
      "epoch   361: best loss = 5.27e-01\n",
      "epoch   363: best loss = 5.25e-01\n",
      "epoch   364: best loss = 5.13e-01\n",
      "epoch   365: best loss = 5.11e-01\n",
      "epoch   366: best loss = 5.09e-01\n",
      "epoch   367: best loss = 5.05e-01\n",
      "epoch   368: best loss = 5.02e-01\n",
      "epoch   369: best loss = 4.97e-01\n",
      "epoch   370: best loss = 4.94e-01\n",
      "epoch   372: best loss = 4.90e-01\n",
      "epoch   373: best loss = 4.77e-01\n",
      "epoch   374: best loss = 4.76e-01\n",
      "epoch   377: best loss = 4.70e-01\n",
      "epoch   378: best loss = 4.70e-01\n",
      "epoch   379: best loss = 4.57e-01\n",
      "epoch   381: best loss = 4.55e-01\n",
      "epoch   383: best loss = 4.54e-01\n",
      "epoch   384: best loss = 4.51e-01\n",
      "epoch   385: best loss = 4.41e-01\n",
      "epoch   387: best loss = 4.37e-01\n",
      "epoch   390: best loss = 4.36e-01\n",
      "epoch   391: best loss = 4.33e-01\n",
      "epoch   392: best loss = 4.28e-01\n",
      "epoch   394: best loss = 4.18e-01\n",
      "epoch   396: best loss = 4.15e-01\n",
      "epoch   397: best loss = 4.11e-01\n",
      "epoch   399: best loss = 4.09e-01\n",
      "epoch   401: best loss = 4.04e-01\n",
      "epoch   403: best loss = 4.01e-01\n",
      "epoch   405: best loss = 4.00e-01\n",
      "epoch   406: best loss = 3.98e-01\n",
      "epoch   407: best loss = 3.96e-01\n",
      "epoch   408: best loss = 3.92e-01\n",
      "epoch   409: best loss = 3.86e-01\n",
      "epoch   411: best loss = 3.81e-01\n",
      "epoch   415: best loss = 3.80e-01\n",
      "epoch   417: best loss = 3.74e-01\n",
      "epoch   418: best loss = 3.73e-01\n",
      "epoch   419: best loss = 3.73e-01\n",
      "epoch   420: best loss = 3.71e-01\n",
      "epoch   422: best loss = 3.69e-01\n",
      "epoch   424: best loss = 3.65e-01\n",
      "epoch   428: best loss = 3.60e-01\n",
      "epoch   432: best loss = 3.58e-01\n",
      "epoch   433: best loss = 3.54e-01\n",
      "epoch   434: best loss = 3.50e-01\n",
      "epoch   437: best loss = 3.49e-01\n",
      "epoch   439: best loss = 3.48e-01\n",
      "epoch   441: best loss = 3.46e-01\n",
      "epoch   442: best loss = 3.42e-01\n",
      "epoch   443: best loss = 3.39e-01\n",
      "epoch   449: best loss = 3.36e-01\n",
      "epoch   451: best loss = 3.34e-01\n",
      "epoch   453: best loss = 3.31e-01\n",
      "epoch   454: best loss = 3.31e-01\n",
      "epoch   455: best loss = 3.30e-01\n",
      "epoch   456: best loss = 3.30e-01\n",
      "epoch   458: best loss = 3.28e-01\n",
      "epoch   459: best loss = 3.27e-01\n",
      "epoch   463: best loss = 3.25e-01\n",
      "epoch   465: best loss = 3.22e-01\n",
      "epoch   466: best loss = 3.19e-01\n",
      "epoch   469: best loss = 3.17e-01\n",
      "epoch   473: best loss = 3.12e-01\n",
      "epoch   476: best loss = 3.10e-01\n",
      "epoch   478: best loss = 3.09e-01\n",
      "epoch   481: best loss = 3.06e-01\n",
      "epoch   484: best loss = 3.06e-01\n",
      "epoch   486: best loss = 3.06e-01\n",
      "epoch   488: best loss = 2.99e-01\n",
      "epoch   489: best loss = 2.94e-01\n",
      "epoch   491: best loss = 2.92e-01\n",
      "epoch   495: best loss = 2.88e-01\n",
      "epoch   500: current loss = 2.91e-01\n",
      "epoch   503: best loss = 2.85e-01\n",
      "epoch   506: best loss = 2.84e-01\n",
      "epoch   508: best loss = 2.83e-01\n",
      "epoch   510: best loss = 2.81e-01\n",
      "epoch   512: best loss = 2.76e-01\n",
      "epoch   513: best loss = 2.76e-01\n",
      "epoch   517: best loss = 2.75e-01\n",
      "epoch   518: best loss = 2.73e-01\n",
      "epoch   521: best loss = 2.71e-01\n",
      "epoch   525: best loss = 2.71e-01\n",
      "epoch   526: best loss = 2.70e-01\n",
      "epoch   527: best loss = 2.68e-01\n",
      "epoch   533: best loss = 2.63e-01\n",
      "epoch   535: best loss = 2.59e-01\n",
      "epoch   540: best loss = 2.59e-01\n",
      "epoch   545: best loss = 2.57e-01\n",
      "epoch   546: best loss = 2.53e-01\n",
      "epoch   552: best loss = 2.50e-01\n",
      "epoch   556: best loss = 2.46e-01\n",
      "epoch   562: best loss = 2.44e-01\n",
      "epoch   565: best loss = 2.42e-01\n",
      "epoch   569: best loss = 2.38e-01\n",
      "epoch   578: best loss = 2.35e-01\n",
      "epoch   585: best loss = 2.34e-01\n",
      "epoch   586: best loss = 2.33e-01\n",
      "epoch   589: best loss = 2.32e-01\n",
      "epoch   590: best loss = 2.32e-01\n",
      "epoch   593: best loss = 2.27e-01\n",
      "epoch   594: best loss = 2.27e-01\n",
      "epoch   602: best loss = 2.23e-01\n",
      "epoch   605: best loss = 2.22e-01\n",
      "epoch   607: best loss = 2.20e-01\n",
      "epoch   611: best loss = 2.20e-01\n",
      "epoch   613: best loss = 2.20e-01\n",
      "epoch   617: best loss = 2.16e-01\n",
      "epoch   618: best loss = 2.15e-01\n",
      "epoch   619: best loss = 2.14e-01\n",
      "epoch   620: best loss = 2.13e-01\n",
      "epoch   623: best loss = 2.13e-01\n",
      "epoch   626: best loss = 2.11e-01\n",
      "epoch   631: best loss = 2.10e-01\n",
      "epoch   635: best loss = 2.08e-01\n",
      "epoch   638: best loss = 2.08e-01\n",
      "epoch   639: best loss = 2.05e-01\n",
      "epoch   640: best loss = 2.05e-01\n",
      "epoch   647: best loss = 2.02e-01\n",
      "epoch   651: best loss = 2.02e-01\n",
      "epoch   655: best loss = 1.96e-01\n",
      "epoch   657: best loss = 1.95e-01\n",
      "epoch   665: best loss = 1.91e-01\n",
      "epoch   672: best loss = 1.89e-01\n",
      "epoch   681: best loss = 1.88e-01\n",
      "epoch   682: best loss = 1.88e-01\n",
      "epoch   683: best loss = 1.84e-01\n",
      "epoch   687: best loss = 1.83e-01\n",
      "epoch   691: best loss = 1.82e-01\n",
      "epoch   693: best loss = 1.80e-01\n",
      "epoch   702: best loss = 1.79e-01\n",
      "epoch   703: best loss = 1.77e-01\n",
      "epoch   707: best loss = 1.72e-01\n",
      "epoch   718: best loss = 1.72e-01\n",
      "epoch   725: best loss = 1.71e-01\n",
      "epoch   729: best loss = 1.71e-01\n",
      "epoch   730: best loss = 1.70e-01\n",
      "epoch   731: best loss = 1.69e-01\n",
      "epoch   733: best loss = 1.67e-01\n",
      "epoch   735: best loss = 1.67e-01\n",
      "epoch   736: best loss = 1.65e-01\n",
      "epoch   740: best loss = 1.65e-01\n",
      "epoch   744: best loss = 1.64e-01\n",
      "epoch   746: best loss = 1.63e-01\n",
      "epoch   753: best loss = 1.62e-01\n",
      "epoch   754: best loss = 1.58e-01\n",
      "epoch   759: best loss = 1.57e-01\n",
      "epoch   768: best loss = 1.56e-01\n",
      "epoch   770: best loss = 1.54e-01\n",
      "epoch   775: best loss = 1.52e-01\n",
      "epoch   783: best loss = 1.50e-01\n",
      "epoch   786: best loss = 1.50e-01\n",
      "epoch   787: best loss = 1.50e-01\n",
      "epoch   789: best loss = 1.50e-01\n",
      "epoch   790: best loss = 1.48e-01\n",
      "epoch   795: best loss = 1.45e-01\n",
      "epoch   802: best loss = 1.45e-01\n",
      "epoch   805: best loss = 1.44e-01\n",
      "epoch   806: best loss = 1.43e-01\n",
      "epoch   808: best loss = 1.40e-01\n",
      "epoch   816: best loss = 1.40e-01\n",
      "epoch   817: best loss = 1.39e-01\n",
      "epoch   819: best loss = 1.39e-01\n",
      "epoch   822: best loss = 1.39e-01\n",
      "epoch   823: best loss = 1.37e-01\n",
      "epoch   825: best loss = 1.36e-01\n",
      "epoch   829: best loss = 1.35e-01\n",
      "epoch   833: best loss = 1.34e-01\n",
      "epoch   835: best loss = 1.33e-01\n",
      "epoch   839: best loss = 1.31e-01\n",
      "epoch   842: best loss = 1.31e-01\n",
      "epoch   847: best loss = 1.30e-01\n",
      "epoch   848: best loss = 1.29e-01\n",
      "epoch   852: best loss = 1.27e-01\n",
      "epoch   856: best loss = 1.27e-01\n",
      "epoch   858: best loss = 1.26e-01\n",
      "epoch   861: best loss = 1.25e-01\n",
      "epoch   865: best loss = 1.25e-01\n",
      "epoch   867: best loss = 1.23e-01\n",
      "epoch   870: best loss = 1.23e-01\n",
      "epoch   873: best loss = 1.22e-01\n",
      "epoch   874: best loss = 1.20e-01\n",
      "epoch   878: best loss = 1.20e-01\n",
      "epoch   881: best loss = 1.19e-01\n",
      "epoch   883: best loss = 1.18e-01\n",
      "epoch   886: best loss = 1.18e-01\n",
      "epoch   890: best loss = 1.17e-01\n",
      "epoch   894: best loss = 1.16e-01\n",
      "epoch   898: best loss = 1.14e-01\n",
      "epoch   902: best loss = 1.13e-01\n",
      "epoch   912: best loss = 1.13e-01\n",
      "epoch   913: best loss = 1.12e-01\n",
      "epoch   917: best loss = 1.10e-01\n",
      "epoch   919: best loss = 1.09e-01\n",
      "epoch   927: best loss = 1.09e-01\n",
      "epoch   928: best loss = 1.08e-01\n",
      "epoch   933: best loss = 1.08e-01\n",
      "epoch   934: best loss = 1.07e-01\n",
      "epoch   939: best loss = 1.05e-01\n",
      "epoch   946: best loss = 1.04e-01\n",
      "epoch   947: best loss = 1.04e-01\n",
      "epoch   948: best loss = 1.03e-01\n",
      "epoch   952: best loss = 1.02e-01\n",
      "epoch   954: best loss = 1.01e-01\n",
      "epoch   962: best loss = 9.90e-02\n",
      "epoch   968: best loss = 9.89e-02\n",
      "epoch   970: best loss = 9.80e-02\n",
      "epoch   971: best loss = 9.80e-02\n",
      "epoch   975: best loss = 9.74e-02\n",
      "epoch   976: best loss = 9.71e-02\n",
      "epoch   977: best loss = 9.67e-02\n",
      "epoch   979: best loss = 9.52e-02\n",
      "epoch   982: best loss = 9.50e-02\n",
      "epoch   991: best loss = 9.31e-02\n",
      "epoch   995: best loss = 9.27e-02\n",
      "epoch   996: best loss = 9.17e-02\n",
      "epoch  1000: current loss = 9.32e-02\n",
      "epoch  1006: best loss = 9.16e-02\n",
      "epoch  1009: best loss = 9.06e-02\n",
      "epoch  1013: best loss = 8.93e-02\n",
      "epoch  1027: best loss = 8.85e-02\n",
      "epoch  1030: best loss = 8.84e-02\n",
      "epoch  1033: best loss = 8.66e-02\n",
      "epoch  1043: best loss = 8.65e-02\n",
      "epoch  1045: best loss = 8.59e-02\n",
      "epoch  1047: best loss = 8.48e-02\n",
      "epoch  1051: best loss = 8.43e-02\n",
      "epoch  1052: best loss = 8.37e-02\n",
      "epoch  1055: best loss = 8.35e-02\n",
      "epoch  1060: best loss = 8.31e-02\n",
      "epoch  1066: best loss = 8.17e-02\n",
      "epoch  1067: best loss = 8.13e-02\n",
      "epoch  1073: best loss = 8.12e-02\n",
      "epoch  1076: best loss = 8.10e-02\n",
      "epoch  1083: best loss = 8.05e-02\n",
      "epoch  1088: best loss = 7.94e-02\n",
      "epoch  1093: best loss = 7.78e-02\n",
      "epoch  1102: best loss = 7.78e-02\n",
      "epoch  1107: best loss = 7.67e-02\n",
      "epoch  1109: best loss = 7.66e-02\n",
      "epoch  1110: best loss = 7.65e-02\n",
      "epoch  1114: best loss = 7.54e-02\n",
      "epoch  1123: best loss = 7.43e-02\n",
      "epoch  1129: best loss = 7.43e-02\n",
      "epoch  1131: best loss = 7.43e-02\n",
      "epoch  1135: best loss = 7.30e-02\n",
      "epoch  1138: best loss = 7.29e-02\n",
      "epoch  1146: best loss = 7.28e-02\n",
      "epoch  1148: best loss = 7.23e-02\n",
      "epoch  1151: best loss = 7.21e-02\n",
      "epoch  1154: best loss = 7.21e-02\n",
      "epoch  1156: best loss = 7.20e-02\n",
      "epoch  1157: best loss = 7.18e-02\n",
      "epoch  1159: best loss = 7.15e-02\n",
      "epoch  1163: best loss = 7.13e-02\n",
      "epoch  1164: best loss = 7.10e-02\n",
      "epoch  1166: best loss = 6.95e-02\n",
      "epoch  1181: best loss = 6.83e-02\n",
      "epoch  1185: best loss = 6.73e-02\n",
      "epoch  1198: best loss = 6.63e-02\n",
      "epoch  1199: best loss = 6.59e-02\n",
      "epoch  1209: best loss = 6.54e-02\n",
      "epoch  1212: best loss = 6.51e-02\n",
      "epoch  1218: best loss = 6.48e-02\n",
      "epoch  1222: best loss = 6.44e-02\n",
      "epoch  1227: best loss = 6.35e-02\n",
      "epoch  1230: best loss = 6.25e-02\n",
      "epoch  1241: best loss = 6.20e-02\n",
      "epoch  1251: best loss = 6.20e-02\n",
      "epoch  1254: best loss = 6.11e-02\n",
      "epoch  1255: best loss = 6.07e-02\n",
      "epoch  1267: best loss = 6.07e-02\n",
      "epoch  1271: best loss = 5.96e-02\n",
      "epoch  1279: best loss = 5.92e-02\n",
      "epoch  1286: best loss = 5.87e-02\n",
      "epoch  1287: best loss = 5.84e-02\n",
      "epoch  1294: best loss = 5.74e-02\n",
      "epoch  1298: best loss = 5.69e-02\n",
      "epoch  1305: best loss = 5.68e-02\n",
      "epoch  1316: best loss = 5.65e-02\n",
      "epoch  1321: best loss = 5.65e-02\n",
      "epoch  1322: best loss = 5.57e-02\n",
      "epoch  1328: best loss = 5.53e-02\n",
      "epoch  1334: best loss = 5.51e-02\n",
      "epoch  1336: best loss = 5.49e-02\n",
      "epoch  1337: best loss = 5.44e-02\n",
      "epoch  1341: best loss = 5.40e-02\n",
      "epoch  1353: best loss = 5.27e-02\n",
      "epoch  1355: best loss = 5.25e-02\n",
      "epoch  1365: best loss = 5.20e-02\n",
      "epoch  1370: best loss = 5.17e-02\n",
      "epoch  1381: best loss = 5.17e-02\n",
      "epoch  1382: best loss = 5.15e-02\n",
      "epoch  1383: best loss = 5.12e-02\n",
      "epoch  1390: best loss = 5.04e-02\n",
      "epoch  1391: best loss = 4.97e-02\n",
      "epoch  1408: best loss = 4.91e-02\n",
      "epoch  1415: best loss = 4.90e-02\n",
      "epoch  1421: best loss = 4.89e-02\n",
      "epoch  1424: best loss = 4.87e-02\n",
      "epoch  1429: best loss = 4.83e-02\n",
      "epoch  1430: best loss = 4.75e-02\n",
      "epoch  1441: best loss = 4.73e-02\n",
      "epoch  1443: best loss = 4.65e-02\n",
      "epoch  1451: best loss = 4.64e-02\n",
      "epoch  1461: best loss = 4.58e-02\n",
      "epoch  1470: best loss = 4.58e-02\n",
      "epoch  1473: best loss = 4.54e-02\n",
      "epoch  1477: best loss = 4.51e-02\n",
      "epoch  1486: best loss = 4.50e-02\n",
      "epoch  1489: best loss = 4.40e-02\n",
      "epoch  1496: best loss = 4.38e-02\n",
      "epoch  1499: current loss = 4.54e-02\n",
      "network loaded successfully\n"
     ]
    }
   ],
   "source": [
    "epocs = 1500\n",
    "n_collocation = 100000\n",
    "n_data = 90000\n",
    "filename = \"sol14\"\n",
    "\n",
    "\n",
    "params = Parameters(1., file_name=filename)\n",
    "network = Network(params)\n",
    "network.train(epocs, n_collocation, n_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi Lagrangian solver class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The semi-lagrangian scheme is given by the following formula:\n",
    "\n",
    "$$\n",
    "u\\left(t_{n+1}, x_i\\right)=\\mathcal{I}_h^m\\left(u\\left(t_n, x\\right)\\right)\\left(x_i-a \\Delta t\\right)\n",
    "$$\n",
    "\n",
    "The SemiLagrangianSolver class implements a numerical solver for simple transport equation using the Semi-Lagrangian method.\n",
    "\n",
    "- `__init__(xmin, xmax, tmin, tmax, nx, nt, a, order, network)`: The constructor method initializes the solver with the given parameters. It sets up the spatial and temporal meshes, initializes solution arrays, and stores the necessary values for solving the transport equation. It also takes a network argument, which represents a neural network used for deep interpolation.\n",
    "\n",
    "- `u_0(x)`: This method defines the initial condition u(t=0, x) for the transport equation. It returns the value of the initial condition at a given point x.\n",
    "\n",
    "- `explicit_solution(x, t)`: This method calculates the exact solution of the transport equation at a given spatial point x and time t.\n",
    "\n",
    "- `find_closest(index)`: This method calculates the order+1 closest points to a given point x_star (phantom point) based on the x_i point and returns the indexes found and the x_star point.\n",
    "\n",
    "- `Li(x_star, x_closest, i)`: This method calculates the Lagrange basis polynomial L_i at x_star for the i-th closest point x_closest.\n",
    "\n",
    "- `solver()`: This method solves the transport equation numerically using the Lagrange interpolation operator. It iterates over time and space to calculate the solution at each time step and spatial point.\n",
    "\n",
    "- `u_theta(x, t)`: This method calculates the solution $u_\\theta$ obtained by the neural network. It takes a spatial point x and time t as inputs and returns the predicted solution.\n",
    "\n",
    "The Deep Lagrange interpolator is given by\n",
    "\n",
    "$$\n",
    "\\mathcal{I}_d^m(f)=\\sum_{i=1}^n \\frac{f\\left(x_i\\right)}{u_{\\theta_i}\\left(x_i\\right)} P_i(x) u_{\\theta_i}(x)\n",
    "$$\n",
    "\n",
    "with $P_i\\left(x_j\\right)=\\delta_{i j}$.\n",
    "\n",
    "Using this choice, we obtain that $\\mathcal{I}_d(f)\\left(x_i\\right)=f\\left(x_i\\right)$ as the classical interpolator.\n",
    "\n",
    "- `solver_deep()`: This method solves the transport equation numerically using the deep Lagrange interpolation operator. It iterates over time and space to calculate the solution at each time step and spatial point using the u_theta method.\n",
    "\n",
    "- `plot_solution(t)`: This method plots the numerical solution at a given time t along with the exact solution.\n",
    "\n",
    "- `plot_solution_deep(t)`: This method plots the numerical solution obtained using deep interpolation at a given time t along with the exact solution and the solution obtained using the Lagrange interpolation operator.\n",
    "\n",
    "- `error_solution(t)`: This method calculates the error of the numerical solution compared to the exact solution at a given time t.\n",
    "\n",
    "- `error_inf_norm(deep)`: This method calculates the error in the infinity norm for both solutions (Lagrange interpolation and deep interpolation).\n",
    "\n",
    "- `error_L2_norm(deep)`: This method calculates the error in the L2 norm for both solutions.\n",
    "\n",
    "- `error_solution_deep(t)`: This method calculates the error of the numerical solution obtained using deep interpolation compared to the exact solution at a given time t.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A function that will create the mesh\"\"\"\n",
    "\n",
    "def mesh (xmin, xmax, tmin, tmax, nx, nt):\n",
    "    x = np.linspace(xmin, xmax, nx, endpoint=True)\n",
    "    t = np.linspace(tmin, tmax, nt, endpoint=True)\n",
    "    dx = x[1] - x[0]\n",
    "    dt = t[1] - t[0]\n",
    "    return x, t, dx, dt\n",
    "\n",
    "class SemiLagrangianSolver:\n",
    "\n",
    "    def __init__(self, xmin, xmax, tmin, tmax, nx, nt, a, order,network):\n",
    "        \n",
    "        self.xmesh, self.tmesh, self.delta_x, self.delta_t = mesh(xmin, xmax, tmin, tmax, nx, nt)\n",
    "\n",
    "        self.xmax = xmax\n",
    "        self.xmin = xmin\n",
    "        self.tmax = tmax\n",
    "        self.a = a #velocity\n",
    "        self.nt=nt\n",
    "        self.nx=nx\n",
    "        self.u = np.zeros((nt, nx))\n",
    "        self.u_deep = np.zeros((nt, nx))\n",
    "        self.order=order\n",
    "        self.mean = 0.479\n",
    "        self.variance = 0.09\n",
    "        self.network = network\n",
    "    \n",
    "    \"\"\"Function that defines the u(t=0, x) initial condition\"\"\"\n",
    "\n",
    "    def u_0(self, x):\n",
    "        return np.exp(-(x-self.mean)**2/self.variance)\n",
    "    \n",
    "\n",
    "    \"\"\"Function that calculates the explicit solution of the transport equation\"\"\"\n",
    "\n",
    "    def explicit_solution(self, x, t): \n",
    "        return self.u_0((x - self.a * t) % self.xmax)\n",
    "\n",
    "\n",
    "    \"\"\"Function that calculates the closest points to the point x_star (phantom point) \n",
    "       based on the x_i point and returns the indexes and the x_star point\"\"\"\n",
    "\n",
    "    def find_closest(self, index):\n",
    "        # We first calculate the x_star point\n",
    "        x_i = self.xmesh[index]\n",
    "        # We take into account the periodicity of the domain\n",
    "        x_star = (x_i - self.a * self.delta_t) % self.xmax\n",
    "    \n",
    "        # We calculate the k index so that x_star is in between x_k and x_{k+1}\n",
    "        k = int(x_star / self.delta_x)\n",
    "        #We calculate the other indexes based on k and the order\n",
    "        indexes = np.zeros(self.order+1, dtype=int)\n",
    "        indexes = np.arange(k - (self.order//2), k + (self.order//2) + 2)\n",
    "        indexes = np.mod(indexes, self.nx)\n",
    "        return indexes, x_star\n",
    "\n",
    "\n",
    "    \"\"\"Function that calculates the Lagrange basis polynomial Li at x_star\"\"\"\n",
    "    \n",
    "    def Li(self, x_star, x_closest, i):\n",
    "        result = 1.0\n",
    "        for j in range(len(x_closest)):\n",
    "            if j != i:\n",
    "                result *= (x_star - x_closest[j]) / (x_closest[i] - x_closest[j])\n",
    "        return result\n",
    "\n",
    "    \"\"\"Function that calculates the numerical solution of the Transport equation \n",
    "       using de Lagrange interpolation operator\"\"\"\n",
    " \n",
    "    def solver(self): #We calculate for t+1 at index index i the mesh at that time \n",
    "\n",
    "        self.u[0,:] = self.u_0(self.xmesh)\n",
    "        \n",
    "        #We calculate the solution for each time step and each x_i point\n",
    "        for t in range(self.nt-1):\n",
    "            for i in range(self.nx):\n",
    "                # We calculate the closest points to x_star\n",
    "                closest_indexes, x_star = self.find_closest(i)\n",
    "                # We retrieve the closest points in our mesh\n",
    "                x_closest = self.xmesh[closest_indexes]\n",
    "                # We retrieve the solution at the closest pointsat the time t\n",
    "                u_closest = self.u[t, closest_indexes]\n",
    "                li = np.zeros(self.order+1)\n",
    "                for j in range(self.order+1):\n",
    "                    li[j] = self.Li(x_star, x_closest, j)\n",
    "\n",
    "                u_sol = np.sum(u_closest*li)\n",
    "\n",
    "                self.u[t+1,i] = u_sol   \n",
    "\n",
    "        return self.u\n",
    "    \n",
    "\n",
    "    \"\"\"u_thetai function\"\"\" \n",
    "    def u_theta(self, x, t):\n",
    "        pred= self.network.predict_u_from_torch(x, t, self.mean, self.variance)\n",
    "        return pred.detach().cpu().numpy() \n",
    "\n",
    "    \"\"\"Function that calculates using the Deep Lagrange interpolation the numerical solution of the transport equation\"\"\"\n",
    "\n",
    "    def solver_deep(self):\n",
    "        self.u_deep[0,:] = self.u_0(self.xmesh)\n",
    "\n",
    "        for t in range(self.nt-1):\n",
    "            for i in range(self.nx):\n",
    "    \n",
    "                closest_indexes, x_star = self.find_closest(i) #We get the closest indexes \n",
    "        \n",
    "                x_closest = self.xmesh[closest_indexes] #We get the values of the mesh at those closest indexes\n",
    "                x_closest_tensor = torch.tensor(x_closest.reshape(-1, 1), dtype=torch.float32).clone().detach() #we convert the array into a tensor\n",
    "            \n",
    "                u_closest = self.u_deep[t][closest_indexes] #We get the values of the solution at those closest indexes at time t\n",
    "                u_theta_tensor = self.u_theta(x_closest_tensor, self.tmesh[t]) #We calcualte \n",
    "\n",
    "                #We convert the tensor into an array\n",
    "                u_theta_i=[]\n",
    "                for k in range(len(u_theta_tensor)):\n",
    "                    u_theta_i.append(u_theta_tensor[k].item())\n",
    "\n",
    "                u_closest_theta = u_closest / u_theta_i\n",
    "        \n",
    "                li = np.zeros(self.order+1)\n",
    "                for j in range(self.order+1):\n",
    "        \n",
    "                    li[j] = self.Li(x_star, x_closest, j)\n",
    "            \n",
    "                    x_star_tensor = torch.tensor(x_star.reshape(-1, 1), dtype=torch.float32).clone().detach() #We convert x_star into a tensor \n",
    "                    u_theta_tensor = self.u_theta(x_star_tensor, self.tmesh[t]) #We\n",
    "                    u_theta = u_theta_tensor.item() #We get the floar out of the the tensor\n",
    "                    li[j] *= u_theta\n",
    "                \n",
    "                u_sol = np.sum(u_closest_theta * li)\n",
    "\n",
    "                self.u_deep[t+1][i] = u_sol\n",
    "\n",
    "        return self.u_deep\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"Function that will show the plot of the solution at a given time t\"\"\"\n",
    "  \n",
    "    def plot_solution(self,t):\n",
    "        plt.plot(self.xmesh, self.u[t,:], label = 'numerical solution')\n",
    "        plt.plot(self.xmesh, self.explicit_solution(self.xmesh, self.tmesh[t]), label = 'exact solution')\n",
    "        plt.title(\"Solution at time t = \" + str(self.tmesh[t]))\n",
    "        plt.grid()\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(\"numerical solution\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "    \"\"\"Function that will show the plot of the solution using deep interpolation at a given time t\"\"\"\n",
    "    def plot_solution_deep(self,t):\n",
    "        plt.plot(self.xmesh, self.u_deep[t,:], label = 'u deep with PINNs')\n",
    "        #print(\"deep\",self.u_deep[t,:])\n",
    "        #xmesh_tensor = torch.tensor(self.xmesh.reshape(-1, 1), dtype=torch.float32).clone().detach()\n",
    "        #plt.plot(self.xmesh, self.u_theta(xmesh_tensor, self.tmesh[t]), label = 'sol donné par le reseau')\n",
    "        plt.plot(self.xmesh, self.explicit_solution(self.xmesh, self.tmesh[t]), label = 'exact solution')\n",
    "        #print(\"exact\",self.explicit_solution(self.xmesh, self.tmesh[t]))\n",
    "        plt.plot(self.xmesh, self.u[t,:], label = 'u')\n",
    "        #print(\"u\",self.u[t,:])\n",
    "        plt.title(\"Solution at time t = \" + str(self.tmesh[t]))\n",
    "        plt.grid()\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(\"numerical solution deep\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "    \"\"\" Function that will calculate the error of the numeral solution compared to the analytical solution at a given time t\"\"\"\n",
    "    def error_solution(self, t):\n",
    "\n",
    "        if t > 0:\n",
    "            return np.sum(np.linalg.norm(self.u[t,:] - self.explicit_solution(self.xmesh, self.tmesh[t]), ord=2))\n",
    "        else:\n",
    "            return 0\n",
    "     \n",
    "    \"\"\"Function that will calculate the error in infinity norm for both solutions\"\"\"\n",
    "    def error_inf_norm(self):\n",
    "        diff = 0\n",
    "        for t in range(1,self.nt):\n",
    "            diff = np.abs(self.u[t,:] - self.explicit_solution(self.xmesh, self.tmesh[t]))\n",
    "            norm = np.max(diff)\n",
    "        return norm\n",
    "\n",
    "    def error_inf_norm_deep(self):\n",
    "        diff = 0\n",
    "        for t in range(1,self.nt):\n",
    "            diff = np.abs(self.u_deep[t,:] - self.explicit_solution(self.xmesh, self.tmesh[t]))\n",
    "            norm = np.max(diff)\n",
    "        return norm\n",
    "    \n",
    "    \"\"\"Function that will calculate the error in L2 norm for both solutions\"\"\"\n",
    "    def error_L2_norm_deep(self):\n",
    "        diff = 0\n",
    "        norm = 0\n",
    "        for t in range(1,self.nt):\n",
    "            diff = self.explicit_solution(self.xmesh, self.tmesh[t])-self.u_deep[t,:]\n",
    "        for j in range(self.nx):\n",
    "            norm += (diff[j-1]**2 + diff[j]**2 )* self.delta_x / 2\n",
    "\n",
    "    def error_L2_norm(self):\n",
    "        diff = 0\n",
    "        norm =0\n",
    "        for t in range(1,self.nt):\n",
    "            diff = self.explicit_solution(self.xmesh, self.tmesh[t])-self.u[t,:]\n",
    "        for j in range(self.nx):\n",
    "            norm += (diff[j-1]**2 + diff[j]**2 )* self.delta_x / 2\n",
    "        return norm\n",
    "\n",
    "    \"\"\" Function that will calculate the error of the numeral deep solution compared to the analytical solution at a given time t\"\"\"\n",
    "    def error_solution_deep(self, t):\n",
    "        if t > 0:\n",
    "            return np.sum(np.linalg.norm(self.u_deep[t,:] - self.explicit_solution(self.xmesh, self.tmesh[t]), ord=2))\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution using PINNs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cfl solver 0.49748743718592964\n"
     ]
    }
   ],
   "source": [
    "\"\"\"We first define the values of the parameters of the solver \"\"\"\n",
    "xmin = 0.\n",
    "xmax = 1.\n",
    "tmin = 0.   \n",
    "tmax = 1.\n",
    "nx = 100\n",
    "nt = 200 \n",
    "a=1\n",
    "         \n",
    "solver = SemiLagrangianSolver(xmin, xmax, tmin, tmax, nx, nt, a, 3, network)\n",
    "X = solver.xmesh\n",
    "T = solver.tmesh\n",
    "\n",
    "\"\"\"We verify the CFL condition\"\"\"\n",
    "\n",
    "print(\"cfl solver\" , a*solver.delta_t/solver.delta_x)\n",
    "\n",
    "\"\"\"Solutions of the problem\"\"\"\n",
    "u_semilag = solver.solver()\n",
    "u_semilag_deep = solver.solver_deep()\n",
    "\n",
    "\"\"\"Exact solution\"\"\"\n",
    "u_exact = np.zeros((solver.nt, solver.nx))\n",
    "for i in range(solver.nt):\n",
    "    for j in range(solver.nx):\n",
    "        u_exact[i,j] = solver.explicit_solution(X[j], T[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "line_u_semilag, = ax.plot([], color='blue', label='u')\n",
    "line_u_semilag_deep, = ax.plot([], color='green', label='u_deep')\n",
    "line_exact, = ax.plot([], color='red', label='u_exact')\n",
    "ax.grid()\n",
    "ax.set_xlim(0, 1.)\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.legend()\n",
    "\n",
    "def animate(frame_num):\n",
    "    line_u_semilag.set_data(X, u_semilag[frame_num,:])\n",
    "    line_u_semilag_deep.set_data(X, u_semilag_deep[frame_num,:])\n",
    "    line_exact.set_data(X, u_exact[frame_num,:])\n",
    "    return line_u_semilag, line_u_semilag_deep, line_exact\n",
    "\n",
    "anim_bis = FuncAnimation(fig, animate, frames=solver.nt, interval=120, blit=True)\n",
    "anim_bis.save(\"animation\" + str(solver.mean) + \".gif\", writer='pillow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphique Deep solution vs Exact solution vs Solution numérique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = list(range(solver.nt)) \n",
    "# selected_indices = indices[::20]  \n",
    "\n",
    "# for i in range(solver.nt):\n",
    "#     solver.plot_solution_deep(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vérification de la solution prédite par le réseau de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis - Study of the convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.029136599892419242\n"
     ]
    }
   ],
   "source": [
    "#Initial condition error \n",
    " \n",
    "print(np.sum(np.linalg.norm(u_semilag[0,:] - solver.explicit_solution(solver.xmesh, solver.tmesh[0]), ord=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time= 0\n",
      "error 0\n",
      "error deep 0\n",
      "time= 1\n",
      "error 7.214175577180251e-06\n",
      "error deep 4.1444664652392346e-07\n",
      "time= 2\n",
      "error 0.008323666668382405\n",
      "error deep 0.008775665524213086\n",
      "time= 3\n",
      "error 0.010667550591531176\n",
      "error deep 0.010499902391513992\n",
      "time= 4\n",
      "error 0.008676967379567623\n",
      "error deep 0.009139669748584855\n",
      "time= 5\n",
      "error 0.011972725800063675\n",
      "error deep 0.01181299354128456\n",
      "time= 6\n",
      "error 0.009163515741114863\n",
      "error deep 0.009582977781240815\n",
      "time= 7\n",
      "error 0.012885760632330712\n",
      "error deep 0.012687313789337575\n",
      "time= 8\n",
      "error 0.009618766455682486\n",
      "error deep 0.009993315939260897\n",
      "time= 9\n",
      "error 0.0135401166342246\n",
      "error deep 0.013335254687161575\n",
      "time= 10\n",
      "error 0.00996077878750177\n",
      "error deep 0.0103195723669156\n",
      "time= 11\n",
      "error 0.01405344466911201\n",
      "error deep 0.013855326353656713\n",
      "time= 12\n",
      "error 0.01025712976001802\n",
      "error deep 0.010604140038908564\n",
      "time= 13\n",
      "error 0.014491354913029613\n",
      "error deep 0.014302522183543743\n",
      "time= 14\n",
      "error 0.010525475269863107\n",
      "error deep 0.010862221312307395\n",
      "time= 15\n",
      "error 0.01487729380148326\n",
      "error deep 0.01469967327081615\n",
      "time= 16\n",
      "error 0.010771483440781875\n",
      "error deep 0.011100584641321689\n",
      "time= 17\n",
      "error 0.015225034448410153\n",
      "error deep 0.015059927432702389\n",
      "time= 18\n",
      "error 0.010999946906922537\n",
      "error deep 0.011324060671582474\n",
      "time= 19\n",
      "error 0.01554361288914357\n",
      "error deep 0.01539156187174146\n",
      "time= 20\n",
      "error 0.011214243399424702\n",
      "error deep 0.011535872051123826\n",
      "time= 21\n",
      "error 0.015839103283995012\n",
      "error deep 0.01570004379463787\n",
      "time= 22\n",
      "error 0.011416829283583962\n",
      "error deep 0.011738215731716855\n",
      "time= 23\n",
      "error 0.016115831074489596\n",
      "error deep 0.015989207065594598\n",
      "time= 24\n",
      "error 0.011609580477072516\n",
      "error deep 0.011932554306360667\n",
      "time= 25\n",
      "error 0.016376993778683808\n",
      "error deep 0.01626183331188279\n",
      "time= 26\n",
      "error 0.011793956417970862\n",
      "error deep 0.012119778687427363\n",
      "time= 27\n",
      "error 0.016625022723283443\n",
      "error deep 0.0165200444584016\n",
      "time= 28\n",
      "error 0.011971113944975126\n",
      "error deep 0.012300410810836578\n",
      "time= 29\n",
      "error 0.01686181210914602\n",
      "error deep 0.016765557114177775\n",
      "time= 30\n",
      "error 0.012141985849965531\n",
      "error deep 0.012474783810210084\n",
      "time= 31\n",
      "error 0.017088867952414356\n",
      "error deep 0.016999841699979857\n",
      "time= 32\n",
      "error 0.012307335295681173\n",
      "error deep 0.012643188024448736\n",
      "time= 33\n",
      "error 0.017307408431645615\n",
      "error deep 0.017224260271897496\n",
      "time= 34\n",
      "error 0.012467794668086361\n",
      "error deep 0.012806008452881561\n",
      "time= 35\n",
      "error 0.017518433604539377\n",
      "error deep 0.017440078466457804\n",
      "time= 36\n",
      "error 0.012623893906636326\n",
      "error deep 0.01296373816647755\n",
      "time= 37\n",
      "error 0.017722775098560715\n",
      "error deep 0.017648460683306923\n",
      "time= 38\n",
      "error 0.01277608155764266\n",
      "error deep 0.013116958720722079\n",
      "time= 39\n",
      "error 0.017921132332453665\n",
      "error deep 0.017850456146775318\n",
      "time= 40\n",
      "error 0.012924740686510198\n",
      "error deep 0.013266293077257239\n",
      "time= 41\n",
      "error 0.018114099440128376\n",
      "error deep 0.01804698949683991\n",
      "time= 42\n",
      "error 0.013070201082433066\n",
      "error deep 0.013412351206811508\n",
      "time= 43\n",
      "error 0.018302185630519784\n",
      "error deep 0.018238773814411996\n",
      "time= 44\n",
      "error 0.013212748744687725\n",
      "error deep 0.013555614257539567\n",
      "time= 45\n",
      "error 0.01848583082176017\n",
      "error deep 0.018426350367910082\n",
      "time= 46\n",
      "error 0.01335263334603348\n",
      "error deep 0.01369644281180397\n",
      "time= 47\n",
      "error 0.018665417812922098\n",
      "error deep 0.018610145583157465\n",
      "time= 48\n",
      "error 0.013490074169853929\n",
      "error deep 0.01383510497418821\n",
      "time= 49\n",
      "error 0.018841281878384675\n",
      "error deep 0.018790448368789684\n",
      "time= 50\n",
      "error 0.013625264881037083\n",
      "error deep 0.013971743267014102\n",
      "time= 51\n",
      "error 0.019013718415969206\n",
      "error deep 0.018967491338331807\n",
      "time= 52\n",
      "error 0.013758377395241992\n",
      "error deep 0.014106442553789853\n",
      "time= 53\n",
      "error 0.019182989106172832\n",
      "error deep 0.019141480306053428\n",
      "time= 54\n",
      "error 0.013889565043563956\n",
      "error deep 0.014239259308829404\n",
      "time= 55\n",
      "error 0.019349326918682862\n",
      "error deep 0.019312552669507214\n",
      "time= 56\n",
      "error 0.014018965180939947\n",
      "error deep 0.014370195294050443\n",
      "time= 57\n",
      "error 0.01951294021657009\n",
      "error deep 0.019480894129259414\n",
      "time= 58\n",
      "error 0.01414670135120316\n",
      "error deep 0.01449929723694168\n",
      "time= 59\n",
      "error 0.019674016146960918\n",
      "error deep 0.019646682142681348\n",
      "time= 60\n",
      "error 0.014272885095631415\n",
      "error deep 0.014626592986482351\n",
      "time= 61\n",
      "error 0.019832723462173532\n",
      "error deep 0.019810037074906268\n",
      "time= 62\n",
      "error 0.014397617472461086\n",
      "error deep 0.0147520379418006\n",
      "time= 63\n",
      "error 0.01998921488229905\n",
      "error deep 0.019971106604386706\n",
      "time= 64\n",
      "error 0.014520990340299349\n",
      "error deep 0.014875583894153614\n",
      "time= 65\n",
      "error 0.020143629085643242\n",
      "error deep 0.02013003004917223\n",
      "time= 66\n",
      "error 0.014643087447375877\n",
      "error deep 0.014997169209244246\n",
      "time= 67\n",
      "error 0.02029609239496941\n",
      "error deep 0.02028695621632443\n",
      "time= 68\n",
      "error 0.014763985360196602\n",
      "error deep 0.015116787101249092\n",
      "time= 69\n",
      "error 0.020446720213468814\n",
      "error deep 0.020442185918487097\n",
      "time= 70\n",
      "error 0.014883754258742063\n",
      "error deep 0.015234664791671959\n",
      "time= 71\n",
      "error 0.02059561825366179\n",
      "error deep 0.020596165853498532\n",
      "time= 72\n",
      "error 0.01500245862039309\n",
      "error deep 0.01535129969941194\n",
      "time= 73\n",
      "error 0.020742883594151544\n",
      "error deep 0.020749424759629476\n",
      "time= 74\n",
      "error 0.015120157810898337\n",
      "error deep 0.01546740887292233\n",
      "time= 75\n",
      "error 0.02088860559270649\n",
      "error deep 0.02090249368110471\n",
      "time= 76\n",
      "error 0.015236906597647957\n",
      "error deep 0.015583803130293348\n",
      "time= 77\n",
      "error 0.02103286667906538\n",
      "error deep 0.02105566773101957\n",
      "time= 78\n",
      "error 0.015352755598067484\n",
      "error deep 0.015701089080127583\n",
      "time= 79\n",
      "error 0.02117574304681757\n",
      "error deep 0.021208830174420105\n",
      "time= 80\n",
      "error 0.01546775167393748\n",
      "error deep 0.015819423730122763\n",
      "time= 81\n",
      "error 0.021317305260437815\n",
      "error deep 0.02136150428024881\n",
      "time= 82\n",
      "error 0.015581938280751008\n",
      "error deep 0.01593848746347112\n",
      "time= 83\n",
      "error 0.021457618790867813\n",
      "error deep 0.021513009154045915\n",
      "time= 84\n",
      "error 0.015695355779754942\n",
      "error deep 0.01605760694835968\n",
      "time= 85\n",
      "error 0.021596744490789965\n",
      "error deep 0.021662719667452424\n",
      "time= 86\n",
      "error 0.01580804171901087\n",
      "error deep 0.016176021540844156\n",
      "time= 87\n",
      "error 0.0217347390188273\n",
      "error deep 0.02181033077536834\n",
      "time= 88\n",
      "error 0.015920031088622572\n",
      "error deep 0.016293169478336998\n",
      "time= 89\n",
      "error 0.021871655220252712\n",
      "error deep 0.021955873249164093\n",
      "time= 90\n",
      "error 0.016031356554178096\n",
      "error deep 0.016408769686413454\n",
      "time= 91\n",
      "error 0.022007542470354757\n",
      "error deep 0.022099601745248467\n",
      "time= 92\n",
      "error 0.01614204867144906\n",
      "error deep 0.01652278637557418\n",
      "time= 93\n",
      "error 0.022142446985351144\n",
      "error deep 0.02224191126937729\n",
      "time= 94\n",
      "error 0.016252136084471677\n",
      "error deep 0.01663539076256128\n",
      "time= 95\n",
      "error 0.02227641210465237\n",
      "error deep 0.022383202448310782\n",
      "time= 96\n",
      "error 0.0163616457083272\n",
      "error deep 0.016746866795213665\n",
      "time= 97\n",
      "error 0.02240947854734977\n",
      "error deep 0.02252381755308997\n",
      "time= 98\n",
      "error 0.016470602897259067\n",
      "error deep 0.0168575575491135\n",
      "time= 99\n",
      "error 0.02254168464503842\n",
      "error deep 0.02266405557441597\n",
      "time= 100\n",
      "error 0.016579031598232\n",
      "error deep 0.016967841587023625\n",
      "time= 101\n",
      "error 0.022673066552481923\n",
      "error deep 0.022804119543554106\n",
      "time= 102\n",
      "error 0.016686954489667876\n",
      "error deep 0.017078043503809873\n",
      "time= 103\n",
      "error 0.022803658437193577\n",
      "error deep 0.02294409142289991\n",
      "time= 104\n",
      "error 0.01679439310490537\n",
      "error deep 0.017188372292490362\n",
      "time= 105\n",
      "error 0.02293349264873982\n",
      "error deep 0.02308392656148571\n",
      "time= 106\n",
      "error 0.016901367939910712\n",
      "error deep 0.017298892293978723\n",
      "time= 107\n",
      "error 0.023062599868459134\n",
      "error deep 0.023223517500957892\n",
      "time= 108\n",
      "error 0.017007898544924824\n",
      "error deep 0.01740954889357462\n",
      "time= 109\n",
      "error 0.0231910092403212\n",
      "error deep 0.02336272746471797\n",
      "time= 110\n",
      "error 0.017114003600031333\n",
      "error deep 0.017520202889269768\n",
      "time= 111\n",
      "error 0.023318748483798807\n",
      "error deep 0.02350144686691131\n",
      "time= 112\n",
      "error 0.017219700975046767\n",
      "error deep 0.01763069382710074\n",
      "time= 113\n",
      "error 0.02344584398986941\n",
      "error deep 0.02363964851799536\n",
      "time= 114\n",
      "error 0.017325007774629884\n",
      "error deep 0.01774090619763782\n",
      "time= 115\n",
      "error 0.02357232090155493\n",
      "error deep 0.023777390465640418\n",
      "time= 116\n",
      "error 0.017429940370026778\n",
      "error deep 0.017850804176829178\n",
      "time= 117\n",
      "error 0.023698203180722757\n",
      "error deep 0.02391482155912667\n",
      "time= 118\n",
      "error 0.017534514419370106\n",
      "error deep 0.01796047762525676\n",
      "time= 119\n",
      "error 0.023823513663162967\n",
      "error deep 0.024052175576335922\n",
      "time= 120\n",
      "error 0.0176387448788849\n",
      "error deep 0.018070132135179825\n",
      "time= 121\n",
      "error 0.02394827410419323\n",
      "error deep 0.024189723233789928\n",
      "time= 122\n",
      "error 0.017742646007670778\n",
      "error deep 0.018180057003937173\n",
      "time= 123\n",
      "error 0.02407250521718947\n",
      "error deep 0.02432772812546007\n",
      "time= 124\n",
      "error 0.017846231368911482\n",
      "error deep 0.018290579469366523\n",
      "time= 125\n",
      "error 0.024196226707481887\n",
      "error deep 0.02446640975538977\n",
      "time= 126\n",
      "error 0.017949513830369596\n",
      "error deep 0.01840201292106565\n",
      "time= 127\n",
      "error 0.024319457303974768\n",
      "error deep 0.024605898447465784\n",
      "time= 128\n",
      "error 0.018052505566863615\n",
      "error deep 0.0185146099695046\n",
      "time= 129\n",
      "error 0.02444221479064031\n",
      "error deep 0.024746244948028188\n",
      "time= 130\n",
      "error 0.018155218067088245\n",
      "error deep 0.018628576438225703\n",
      "time= 131\n",
      "error 0.02456451603971533\n",
      "error deep 0.024887415575904487\n",
      "time= 132\n",
      "error 0.01825766214665504\n",
      "error deep 0.018744026266089734\n",
      "time= 133\n",
      "error 0.024686377048009547\n",
      "error deep 0.025029293638296153\n",
      "time= 134\n",
      "error 0.018359847968668744\n",
      "error deep 0.018860980113090398\n",
      "time= 135\n",
      "error 0.024807812977296805\n",
      "error deep 0.0251717135090662\n",
      "time= 136\n",
      "error 0.0184617850725364\n",
      "error deep 0.01897938072485968\n",
      "time= 137\n",
      "error 0.024928838199216714\n",
      "error deep 0.02531445945908823\n",
      "time= 138\n",
      "error 0.01856348241089812\n",
      "error deep 0.019099096844356407\n",
      "time= 139\n",
      "error 0.025049466344457084\n",
      "error deep 0.025457309727887584\n",
      "time= 140\n",
      "error 0.018664948393782344\n",
      "error deep 0.019219972666363074\n",
      "time= 141\n",
      "error 0.02516971035559277\n",
      "error deep 0.025600021443808803\n",
      "time= 142\n",
      "error 0.0187661909389879\n",
      "error deep 0.019341818444515747\n",
      "time= 143\n",
      "error 0.025289582543082154\n",
      "error deep 0.025742324768533824\n",
      "time= 144\n",
      "error 0.018867217527729265\n",
      "error deep 0.019464378385891844\n",
      "time= 145\n",
      "error 0.025409094643280363\n",
      "error deep 0.025883890301318168\n",
      "time= 146\n",
      "error 0.018968035262318356\n",
      "error deep 0.019587305186453675\n",
      "time= 147\n",
      "error 0.025528257874833347\n",
      "error deep 0.026024358326230172\n",
      "time= 148\n",
      "error 0.019068650920065905\n",
      "error deep 0.019710175973234594\n",
      "time= 149\n",
      "error 0.02564708299061549\n",
      "error deep 0.026163336532429686\n",
      "time= 150\n",
      "error 0.01916907100507484\n",
      "error deep 0.019832499266105665\n",
      "time= 151\n",
      "error 0.02576558033262626\n",
      "error deep 0.026300484209430364\n",
      "time= 152\n",
      "error 0.01926930181428711\n",
      "error deep 0.01995380891262425\n",
      "time= 153\n",
      "error 0.025883759900608424\n",
      "error deep 0.026435546458207962\n",
      "time= 154\n",
      "error 0.01936934951493084\n",
      "error deep 0.02007370963932672\n",
      "time= 155\n",
      "error 0.026001631407412004\n",
      "error deep 0.02656843899622071\n",
      "time= 156\n",
      "error 0.01946922015648263\n",
      "error deep 0.02019194422844971\n",
      "time= 157\n",
      "error 0.02611920424384319\n",
      "error deep 0.02669921116441744\n",
      "time= 158\n",
      "error 0.01956891953078291\n",
      "error deep 0.020308377950207845\n",
      "time= 159\n",
      "error 0.02623648735358516\n",
      "error deep 0.026828053557570897\n",
      "time= 160\n",
      "error 0.01966845304709336\n",
      "error deep 0.02042300975374007\n",
      "time= 161\n",
      "error 0.026353489298721982\n",
      "error deep 0.02695522549731292\n",
      "time= 162\n",
      "error 0.01976782615158554\n",
      "error deep 0.020535922986907422\n",
      "time= 163\n",
      "error 0.026470218874263738\n",
      "error deep 0.027080992204415316\n",
      "time= 164\n",
      "error 0.01986704538018268\n",
      "error deep 0.020647261483904132\n",
      "time= 165\n",
      "error 0.026586685822747383\n",
      "error deep 0.027205689146528936\n",
      "time= 166\n",
      "error 0.019966118752044265\n",
      "error deep 0.020757275900163265\n",
      "time= 167\n",
      "error 0.026702900489376534\n",
      "error deep 0.02732966779684489\n",
      "time= 168\n",
      "error 0.020065054237626882\n",
      "error deep 0.02086626891681366\n",
      "time= 169\n",
      "error 0.026818871599600495\n",
      "error deep 0.027453249164984136\n",
      "time= 170\n",
      "error 0.0201638544366292\n",
      "error deep 0.020974466574649006\n",
      "time= 171\n",
      "error 0.026934600034164414\n",
      "error deep 0.02757660388686514\n",
      "time= 172\n",
      "error 0.02026250771662637\n",
      "error deep 0.021081816796151617\n",
      "time= 173\n",
      "error 0.02705007837620795\n",
      "error deep 0.027699649566600114\n",
      "time= 174\n",
      "error 0.020361010182367214\n",
      "error deep 0.021188158936326883\n",
      "time= 175\n",
      "error 0.027165330598260217\n",
      "error deep 0.02782245735487379\n",
      "time= 176\n",
      "error 0.02045943112039066\n",
      "error deep 0.02129393897510777\n",
      "time= 177\n",
      "error 0.027280427187593957\n",
      "error deep 0.027945644687659123\n",
      "time= 178\n",
      "error 0.020557813545896735\n",
      "error deep 0.021399994399105737\n",
      "time= 179\n",
      "error 0.027395312256313857\n",
      "error deep 0.028069682687885424\n",
      "time= 180\n",
      "error 0.020655966766484103\n",
      "error deep 0.02150685370527189\n",
      "time= 181\n",
      "error 0.027509920299653257\n",
      "error deep 0.02819534202772693\n",
      "time= 182\n",
      "error 0.020754333171146125\n",
      "error deep 0.02161693647430775\n",
      "time= 183\n",
      "error 0.027625325842842566\n",
      "error deep 0.028325542711565746\n",
      "time= 184\n",
      "error 0.020855296223177347\n",
      "error deep 0.021734105263013715\n",
      "time= 185\n",
      "error 0.027743281322320335\n",
      "error deep 0.02846067881466669\n",
      "time= 186\n",
      "error 0.02095897339474629\n",
      "error deep 0.021852482165476986\n",
      "time= 187\n",
      "error 0.02786043974013672\n",
      "error deep 0.028590757855544918\n",
      "time= 188\n",
      "error 0.02105581847635054\n",
      "error deep 0.02195735282609506\n",
      "time= 189\n",
      "error 0.027968685964943854\n",
      "error deep 0.028711623189066515\n",
      "time= 190\n",
      "error 0.02114363126927177\n",
      "error deep 0.02206728085767405\n",
      "time= 191\n",
      "error 0.028083357761581162\n",
      "error deep 0.028863272860413746\n",
      "time= 192\n",
      "error 0.021277332377124116\n",
      "error deep 0.022265884561538888\n",
      "time= 193\n",
      "error 0.028272437606631465\n",
      "error deep 0.029117611805405414\n",
      "time= 194\n",
      "error 0.021561421985752807\n",
      "error deep 0.022618349475044302\n",
      "time= 195\n",
      "error 0.02859355481868712\n",
      "error deep 0.029468416491032528\n",
      "time= 196\n",
      "error 0.021983837547555618\n",
      "error deep 0.02300334478356664\n",
      "time= 197\n",
      "error 0.028947324924696236\n",
      "error deep 0.029741436327270863\n",
      "time= 198\n",
      "error 0.02226841327891953\n",
      "error deep 0.02310252576865302\n",
      "time= 199\n",
      "error 0.017219480715528094\n",
      "error deep 0.01787051807011827\n"
     ]
    }
   ],
   "source": [
    "#Error calculation\n",
    "for i in range(solver.nt):\n",
    "    print(\"time=\",i) \n",
    "    print(\"error\", solver.error_solution(i))\n",
    "    print(\"error deep\", solver.error_solution_deep(i))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error calculation when $\\Delta x$ converges to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error of the solution u for 50 points is: 0\n",
      "Error of the solution u_deep for 50 points is: 0\n",
      "Error of the solution u for 100 points is: 0\n",
      "Error of the solution u_deep for 100 points is: 0\n",
      "Error of the solution u for 200 points is: 0\n",
      "Error of the solution u_deep for 200 points is: 0\n",
      "Error of the solution u for 300 points is: 0\n",
      "Error of the solution u_deep for 300 points is: 0\n",
      "Error of the solution u for 400 points is: 0\n",
      "Error of the solution u_deep for 400 points is: 0\n",
      "Error of the solution u for 500 points is: 0\n",
      "Error of the solution u_deep for 500 points is: 0\n"
     ]
    }
   ],
   "source": [
    "nb_points= [50, 100, 200, 300, 400, 500, 600, 700]\n",
    "\n",
    "xmin_prime = 0\n",
    "xmax_prime = 1\n",
    "tmin_prime = 0\n",
    "tmax_prime = 1\n",
    "nt_prime = 300\n",
    "a_prime = 0.4\n",
    "general_errors_infinity_u = []\n",
    "general_errors_L2_u = []\n",
    "general_errors_infinity_u_deep = []\n",
    "general_errors_L2_u_deep = []\n",
    "list_h = []\n",
    "\n",
    "error_vals_u = []\n",
    "error_vals_u_deep = []\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "for points in nb_points:\n",
    "\n",
    "    solver_prime = SemiLagrangianSolver(xmin_prime, xmax_prime, tmin_prime, tmax_prime, points, nt_prime, a_prime, 3, network) #Order oF Lagrange interpolation = 2\n",
    "    solver_prime.solver()\n",
    "    solver_prime.solver_deep()\n",
    "    \n",
    "    general_errors_infinity_u.append(solver_prime.error_inf_norm())\n",
    "    general_errors_L2_u.append(solver_prime.error_L2_norm()) \n",
    "\n",
    "    general_errors_infinity_u_deep.append(solver_prime.error_inf_norm_deep())\n",
    "    general_errors_L2_u_deep.append(solver_prime.error_L2_norm_deep())\n",
    "\n",
    "    list_h.append(solver_prime.delta_x)\n",
    "    \n",
    "    error_vals_u.append([])\n",
    "    error_vals_u_deep.append([])\n",
    "\n",
    "    cfl = (a_prime*solver_prime.delta_t)/solver_prime.delta_x\n",
    "    \n",
    "\n",
    "    for t in range(solver_prime.nt):\n",
    "        error_vals_u[-1].append(solver_prime.error_solution(t))\n",
    "        error_vals_u_deep[-1].append(solver_prime.error_solution_deep(t))\n",
    "        \n",
    "    ax[0].plot(solver_prime.tmesh, error_vals_u[-1], label= \"u with {} points\".format(points))\n",
    "    print(\"Error of the solution u for {} points is: {}\".format(points, solver_prime.error_solution(-1)))\n",
    "    ax[1].plot(solver_prime.tmesh, error_vals_u_deep[-1], label= \"u_deep with {} points\".format(points))   \n",
    "    print(\"Error of the solution u_deep for {} points is: {}\".format(points, solver_prime.error_solution_deep(-1))) \n",
    " \n",
    "    ax[0].loglog()\n",
    "    ax[1].loglog()\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].set_xlabel(\"time\")\n",
    "    ax[i].set_ylabel(\"error\")\n",
    "    ax[i].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data has no positive values, and therefore can not be log-scaled.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2387840/3764460408.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloglog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \"\"\"\n\u001b[1;32m    445\u001b[0m     \u001b[0m_warn_if_gui_out_of_main_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_get_backend_mod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib_inline/backend_inline.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     41\u001b[0m             display(\n\u001b[1;32m     42\u001b[0m                 \u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fetch_figure_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             )\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib_inline/backend_inline.py\u001b[0m in \u001b[0;36m_fetch_figure_metadata\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_transparent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_facecolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;31m# the background is transparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         ticksLight = _is_light([label.get_color()\n\u001b[0m\u001b[1;32m    232\u001b[0m                                 \u001b[0;32mfor\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                                 \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib_inline/backend_inline.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    232\u001b[0m                                 \u001b[0;32mfor\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                                 \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                                 for label in axis.get_ticklabels()])\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mticksLight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mticksLight\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mticksLight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0;31m# there are one or more tick labels, all with the same lightness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mget_ticklabels\u001b[0;34m(self, minor, which)\u001b[0m\n\u001b[1;32m   1454\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mminor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1455\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_minorticklabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1456\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_majorticklabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1458\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_majorticklines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mget_majorticklabels\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1411\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_majorticklabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m         \u001b[0;34m\"\"\"Return this Axis' major tick labels, as a list of `~.text.Text`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1413\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1414\u001b[0m         \u001b[0mticks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_major_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m         \u001b[0mlabels1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtick\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mticks\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_update_ticks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1260\u001b[0m         \u001b[0mthe\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mReturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mticks\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mdrawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m         \"\"\"\n\u001b[0;32m-> 1262\u001b[0;31m         \u001b[0mmajor_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_majorticklocs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1263\u001b[0m         \u001b[0mmajor_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmajor_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m         \u001b[0mmajor_ticks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_major_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmajor_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mget_majorticklocs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1482\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_majorticklocs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m         \u001b[0;34m\"\"\"Return this Axis' major tick locations in data coordinates.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_minorticklocs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/ticker.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2339\u001b[0m         \u001b[0;34m\"\"\"Return the locations of the ticks.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m         \u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_view_interval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2341\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtick_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/ticker.py\u001b[0m in \u001b[0;36mtick_values\u001b[0;34m(self, vmin, vmax)\u001b[0m\n\u001b[1;32m   2356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2357\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvmin\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2358\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   2359\u001b[0m                     \u001b[0;34m\"Data has no positive values, and therefore can not be \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2360\u001b[0m                     \"log-scaled.\")\n",
      "\u001b[0;31mValueError\u001b[0m: Data has no positive values, and therefore can not be log-scaled."
     ]
    }
   ],
   "source": [
    "# fig,axs = plt.subplots(2,2,figsize=(10,5),sharex= True)\n",
    "\n",
    "# axs[0][1].plot(list_h, general_errors_L2_u, '-o',label=\"u L2 norm\")\n",
    "# axs[1][1].plot(list_h, general_errors_L2_u_deep, '-o',label=\"u deep L2 norm\")\n",
    "\n",
    "# for i in range(2):\n",
    "#     for j in range(2):\n",
    "#         axs[i][j].set_xlabel(\"delta_x\")\n",
    "#         axs[i][j].set_ylabel(\"error\")\n",
    "#         axs[i][j].legend()\n",
    "#         axs[i][j].loglog()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Calcul de la pente\"\"\"\n",
    "pente_L2 = np.log(general_errors_L2_u[0]/general_errors_L2_u[-1])/np.log(list_h[0]/list_h[-1])\n",
    "\n",
    "print(\"pente L2\", pente_L2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
